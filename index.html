<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="天行健君子以自强不息">
<meta property="og:type" content="website">
<meta property="og:title" content="Air can sing">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Air can sing">
<meta property="og:description" content="天行健君子以自强不息">
<meta property="og:locale">
<meta property="article:author" content="风鸣北川_rigel">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Air can sing</title>
  








<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Air can sing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/11/05/server-config/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/05/server-config/" itemprop="url">server-config</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-11-05T23:13:43+02:00">
                2021-11-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="server-connection">Server Connection</h1>
<h2 id="generate-private-public-key-pairs">Generate private-public key pairs:</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -f ~&#x2F;.ssh&#x2F;new_rsa -C &quot;triton key for jamie&quot;</span><br></pre></td></tr></table></figure>
<h2 id="copy-public-key-to-server">Copy public key to server</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~&#x2F;.ssh&#x2F;new_rsa.pub name@server.addr</span><br></pre></td></tr></table></figure>
<h2 id="login-with-ssh-key-avoid-typing-pwd-every-time">Login with SSH key (avoid typing pwd every time)</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-add ~&#x2F;.ssh&#x2F;new_rsa</span><br></pre></td></tr></table></figure>
<h2 id="modify-config-file-for-more-convenient-jumping">Modify config file for more convenient jumping</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~&#x2F;.ssh&#x2F;config</span><br></pre></td></tr></table></figure>
<p>add the followings:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cluster name</span><br><span class="line">Host xxx</span><br><span class="line">    User LOGIN_NAME</span><br><span class="line">    Hostname server.addr</span><br></pre></td></tr></table></figure>
<h1 id="environment">Environment</h1>
<h2 id="install-conda">Install conda</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;repo.anaconda.com&#x2F;archive&#x2F;Anaconda3-2021.05-Linux-x86_64.sh</span><br><span class="line">bash Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<h2 id="modify-bash">Modify bash</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure>
<p>Add the followings:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;$PATH:anaconda_install_path</span><br></pre></td></tr></table></figure>
<p>run:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/06/21/graph-embedding-0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/06/21/graph-embedding-0/" itemprop="url">graph_embedding</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-06-21T11:40:01+03:00">
                2021-06-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/" itemprop="url" rel="index">
                    <span itemprop="name">paper_reviews</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.6.21</i> </font>
</p>
<hr>
<p>图结构(Graph)广泛存在于现实各种应用场景。如在社交媒体中，用户之间的关注关系构成一个大的社交网络；又如推荐系统中，用户对商品的关注、评分等行为把可以抽象成用户和商品的交互图。然而，现实中的图结构大多都很复杂，如一个社交媒体中可能有几十亿用户、几千万条边；同时，图中的连接关系可能包含复杂的信息，如一个用户对商品的反馈可能是正面的，也可能是负面的。因此，如何有效地对图结构进行建模是学术界和工业界持续关注的焦点。</p>
<img src="/2021/06/21/graph-embedding-0/graph_emb_1.png" class="">
<p>图嵌入(Graph Embedding)为此提供了很好的解决问题的方法。嵌入(Embedding)的思想是：我们希望可以把节点的特征信息嵌入到一个低维空间中，使得有邻接节点在低维空间中的特征向量彼此靠近，以此达到降维和图表示学习的目的。如果我们以数学语言描述图嵌入问题：</p>
<p>给定图<span class="math inline">\(G=(V, E)\)</span>，其中<span class="math inline">\(V\)</span>代表节点(<strong>Vertex</strong>)集合，<span class="math inline">\(E\)</span>代表图中边(<strong>Edge</strong>)集合。定义变量<span class="math inline">\(Type_v\)</span>，<span class="math inline">\(Type_e\)</span>，分别代表图中节点类型集合和边类型集合。对于图<span class="math inline">\(G\)</span>，当 <span class="math inline">\(|Type_v| + |Type_e| = 2\)</span>时，我们称<span class="math inline">\(G\)</span>为同构图(Homogenous Network)，即节点类型和边类型只有一种的图，比如社交网络；对于图<span class="math inline">\(G\)</span>，当 <span class="math inline">\(|Type_v| + |Type_e| \geq 3\)</span> 时，我们称<span class="math inline">\(G\)</span>为异构图(Heterogenous Network)，即节点类型边类型不止有一种的图，比如用户和商品的关系图。</p>
<p>对于图中节点<span class="math inline">\(u \in V\)</span> ，定义节点<span class="math inline">\(u\)</span>的<strong>邻域</strong>(<strong>neighborhood</strong>)为<span class="math inline">\(N_u\)</span>，我们希望找到一种方法<span class="math inline">\(f:u\rightarrow R^{d \times |V|}\)</span>，将节点特征映射到一个低维度的向量空间中，空间中的向量 <span class="math inline">\(e_u \in R^{d \times |V|}\)</span> 能够很好地聚合<span class="math inline">\(u\)</span>的邻域信息，使得 <span class="math inline">\(v \in N_u\)</span>的向量 <span class="math inline">\(e_v \in R^{d \times |V|}\)</span> 在嵌入空间中与<span class="math inline">\(e_u\)</span>尽可能接近。如下图所示，我们把 <span class="math inline">\(R^{d \times |V|}\)</span> 称为嵌入空间(Embedding Space)，<span class="math inline">\(f\)</span>就是我们希望找到的嵌入方法。</p>
<img src="/2021/06/21/graph-embedding-0/graph_emb_2.png" class="">
<p>本文介绍了常见的Graph Embedding算法及其变体，另外还会对Graph Embedding在工业界实际业务场景中的落地情况进行简单的介绍和总结，重点放在NLP领域word2vec模型出现之后以此为基础提出的图嵌入方法。</p>
<p>本文目录如下：</p>
<!-- ## Matrix Factorization

矩阵分解的理论基础来源于数学中的伪奇异值分解(Singular Value Deposition)。之所以是奇异值分解而不是特征值分解，是因为在实际场景中我们要分解的矩阵不一定是方阵。它在推荐系统中有着广泛的应用，对推荐系统熟悉的同学一定记得2006年netflix的那场著名的比赛，矩阵分解在那场比赛中大放异彩。

以user-item-rating这一矩阵为例，矩阵分解将大矩阵$R:m \times n$ 分解为两个小矩阵$U:m \times k$，$I:k \times n$ 的乘积：

$$R = U\times I$$

这其中的k维向量，代表隐因子（Latent Factor）向量。矩阵分解认为，隐因子一定程度上代表了user和item的某种属性。在user-item-rating这一特定的矩阵中，这两个子矩阵可以分别代表user的偏好程度和item的成分属性。以电影评分矩阵为例，隐因子的$K$维可能代表$K$种电影的成分类型，如悬疑成分、科幻成分等等，那么分解得到的用户矩阵$U$的第k维$U_k$则代表了用户对于这一成分的偏好程度，
电影矩阵$I$的第k维$I_k$则代表了电影内这一成分的占比。矩阵分解认为，用户对不同成分的偏好与电影中不同成分的占比的乘积代表了用户对电影的评分。

<img src="/2021/06/21/graph-embedding-0/mf.png" class="">
<p>对于user-item-rating矩阵而言，评分的预测实际上是一个补全矩阵的过程。原本的矩阵中，有一部分有评分，而有一部分没有。因此，矩阵分解的目标是使得最终得到的两个小矩阵的乘积可以很好地补全大矩阵中没有评分的未知项，如何获得这两个最优的小矩阵即如何学习到满意的可以代表user、item属性的嵌入向量矩阵。在应用中，我们可以以评分的部分与预测得到的评分误差最小为基础构造损失函数，再利用随机梯度下降等方法进行模型训练。然而，矩阵分解的方法在大规模数据集下训练会很慢。</p>
<p><strong><em>P.S.</em></strong> 对于矩阵分解的详细介绍这里推荐xx的三篇km文章<a href="">1</a>，<a href="">2</a>，<a href="">3</a>，从原理到应用都介绍得十分精彩。 --&gt;</p>
<!-- 再推荐一篇[博客]()，介绍十分详细具体。 -->
<!-- <hr> -->
<p>13年Google的Mikilov等人提出的Word2vec模型，正式为我们开启了“万物皆可embedding”的时代。Word2vec认为，同一语义上下文的单词存在天然的语义联系，例如：“an example”。因此，它提出利用同一上下文的单词来学习各自的词嵌入表示（word embedding）。相比传统的词袋模型（one-hot词编码），通过Word2vec学习的词向量在情感分析和语义建模等NLP相关任务上表现出了优异的单词表达能力。此外，Word2vec模型也被广泛用于各类任务，例如微软提出的Item2vec模型等。同时，它也是众多graph embedding模型的重要组成部分。因此，本文的第一步需要为大家深入浅出一下Word2vec这一开山大作。</p>
<!-- ## Graph Embeddings -->
<h3 id="word2vec">word2vec</h3>
<p><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">论文原文</a></p>
<p>滑动窗口(<em>Window</em>)是word2vec模型中非常关键的部分，word2vec用其来捕捉语义上下文，并利用滑动操作学习每一个句子序列上单词的embedding。如句子“拜仁是欧冠冠军”由词语“拜仁，是，欧冠，冠军”组成。我们以“欧冠”这个词语为中心，考虑窗口大小为2的词，那么“拜仁，是，冠军”这几个词语就是“欧冠”的上下文。我们把“欧冠”定义为<strong>中心词</strong>(<em>Center Word</em>)，把上下文词语定义为<strong>背景词</strong>(<em>Context Word</em>)。在word2vec中，每个词语都有两个向量，即中心词向量和背景词向量，取决于当前时刻该词语的角色。word2vec提供了两种方法，分别是以中心词向量生成背景词向量概率分布的skip-gram模型，和以背景词向量生成中心词向量概率分布的c-bow模型。由于它们的模型结构是类似的，本文将以skip-gram为例进行进一步的解释。在skip-gram中，word2vec希望通过最大化背景词出现的概率来对中心词进行有效的表示。即当我们的输入的中心词是“欧冠”时，我们希望输出背景词的概率分布列中，“拜仁”，“是”，“冠军”对应位置的概率最大。</p>
<h4 id="skip-gram">skip-gram</h4>
<img src="/2021/06/21/graph-embedding-0/skipgram.png" class="">
<p>如上图所示，skip-gram包含两个权重矩阵，分别为输入层到隐含层之间<span class="math inline">\(W\)</span>和隐含层到输出层之间的<span class="math inline">\(W&#39;\)</span>，前者包含所有词的中心词向量<span class="math inline">\(v\)</span>，后者包含所有词的背景词向量<span class="math inline">\(u\)</span>。由于输入为中心词的one-hot向量，从输入层到隐含层的映射过程，实际上是以中心词的one-hot向量为索引，对矩阵<span class="math inline">\(W\)</span>的查表过程。而隐含层到输出层的映射过程输出了一个词库大小的向量，该向量的每一维度代表了当前中心词向量与该位置背景词向量之间的相似性，以向量内积来度量该相似性，并使用了softmax对该向量进行了归一化。根据softmax的定义，此时输出向量的每一维度便可以看作词库中每一个词出现的概率。skip-gram所做的就是给定中心词最大化当前窗口内上下文词语出现的联合概率：</p>
<p><span class="math display">\[argmax  \frac{1}{T}\prod_{t=1}^T \prod_{-m\leq j\leq m}P(w^{t+j}_o|w^t_c)\]</span></p>
<p>其中，<span class="math inline">\(T\)</span>代表词库大小，<span class="math inline">\(w_c\)</span>为中心词，<span class="math inline">\(w_o\)</span>为背景词，m为窗口大小。<span class="math inline">\(P(w_o|w_c)\)</span> 为给定中心词<span class="math inline">\(w_c\)</span>下背景词<span class="math inline">\(w_o\)</span>出现的概率，定义为一个softmax形式的函数：</p>
<p><span class="math display">\[P(w_o|w_c) = \frac{exp(u_o^Tv_c)}{\sum_{i\in T}exp(u_i^Tv_c)}\]</span></p>
<p>可以看到，在求解背景词的概率分布时，由于背景词可能是词典中的任意一个词，意味着我们要对整个词典大小的词进行遍历，导致这一步对于每一个背景词的概率预测的计算复杂度为<span class="math inline">\(O|T|\)</span>。而现实中，词表的大小往往达到百万甚至千万级别，这样的计算代价显然是不可行的。因此，word2vec中提出了两种优化训练的方法，分别是<strong>负采样</strong>（<strong>negative sampling</strong>）和层序softmax。负采样在实际情况中应用更广，因此本文也只详细介绍一下负采样的思想和原理。对后者感兴趣可以看论文，本节末尾也给出了一篇论文和一个视频作为辅助理解的材料。</p>
<h4 id="负采样">负采样</h4>
<p>负采样认为考虑对所有单词预测的准确度是非必要的，因此避免了对整个词库的遍历，在skip-gram的基础上引入了噪声词集合<span class="math inline">\(K\)</span>。在给定当前中心词<span class="math inline">\(w_c\)</span>时，它假设：</p>
<ul>
<li>背景词<span class="math inline">\(w_o\)</span>出现在当前训练窗口，概率定义为：<span class="math inline">\(P(D=1|w_c, w_o) = \sigma (u_o^T \cdot v_c)\)</span>。</li>
<li>第k个噪声词<span class="math inline">\(w_k\)</span>不在当前训练窗口，概率定义为：<span class="math inline">\(P(D=0|w_c, w_k) = 1 - \sigma (u_k^T \cdot v_c)\)</span>。</li>
</ul>
<p>其中，激活函数定义为<em>sigmoid</em>函数。负采样的目标是最大化背景词出现的概率，同时最大化噪声词不出现的概率。即最大化下述联合概率分布：</p>
<p><span class="math display">\[
P(w_o|w_c) = P(D=1|w_c, w_o) \prod_{k=1}^K P(D=0|w_c, w_k)
\]</span></p>
<p>分别代入两个概率计算公式，优化后的损失函数变为： <span class="math display">\[
L = -log \frac {1} {1+e^{-u_o^T \cdot v_c}} - \sum_{k=1, w_k～P(w)}^K log \frac {1} {1+e^{u_k^T \cdot v_c}}
\]</span></p>
<h4 id="与矩阵分解的可能联系">与矩阵分解的可能联系</h4>
<p>一般，我们都把Word2ve归类为基于神经网络的表示学习模型。然而，从其训练过程我们可以看出，word2vec同样输出了两个矩阵 <span class="math inline">\(W\)</span> 和 <span class="math inline">\(W’\)</span>，那么它和传统的矩阵分解模型是否存在某种联系呢？实际上，Levy和Goldberg在其论文<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf">Neural Word Embedding as Implicit Matrix Factorization</a>一文中得出结论，认为word2vec实际上是针对语料库的word-context的<strong><em>PMI矩阵</em></strong> <strong><em>(Pointwise Mutual Information, 点间互信息)</em></strong> 的矩阵分解。PMI主要用于计算词语间的语义相似度，统计得到的两个词语在文本中同时出现的概率越大关联度就越高。如果语料库中从未出现word-context对，那么它们之间的PMI值为0或负无穷。</p>
<p>Levy和Goldberg的结论是，word2vec就是对PMI矩阵<span class="math inline">\(P\)</span>的隐式分解。基于负采样的skip gram训练过程能够将词语和其上下文信息嵌入到一起，两个向量<span class="math inline">\(u_i, v_c\)</span>的点积 <span class="math inline">\(P_{ic}\)</span> 可以表示它们同时出现的可能性，也就是 <span class="math inline">\(P_{ic}\)</span> 对于的PMI值。即便如此，面对大规模语料，word2vec的训练过程比起矩阵分解要简单许多。</p>
<p><strong><em>P.S.</em></strong> 对word2vec的详细原理介绍推荐<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1411.2738.pdf&amp;xid=25657,15700021,15700124,15700149,15700168,15700186,15700191,15700201,15700208&amp;usg=ALkJrhhNCZKc2CO7hRoTrGd6aH2nBc-ZVQ">Xin Rong的论文</a>，以及YouTube的一个<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=C4X0Cb5_FSo&amp;t=858s">讲解视频</a>，对于其数学原理的细节部分介绍的十分详细。</p>
<p>回归正题。如果我们想将词向量的训练方法应用到其他的场景中，那么需要明确的有两点：</p>
<ul>
<li>如何构造有意义的“上下文信息”。在词向量的训练模型中，上下文信息指的是语料库中的所有句子。这些句子提供了词语之间的上下文关系，是学习语义相似性的基础。</li>
<li>如何结合具体应用场景对word2vec进行修改，使其与实际业务更加匹配。</li>
</ul>
<h3 id="工业界应用-airbnb---real-time-personalization-using-embeddings-for-search-ranking-at-airbnb">工业界应用: Airbnb - Real-time Personalization using Embeddings for Search Ranking at Airbnb</h3>
<p><a target="_blank" rel="noopener" href="https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb">论文原文</a></p>
<p>Airbnb的这篇论文获得了KDD’ 2018 best paper，是word2vec结合业务场景改进的一次成功实践。它的模型简单不花哨，但工程性极强 ，融入了平台自身的业务特点，也取得了很好的落地效果，值得仔细阅读和思考。</p>
<p>Airbnb是一个民宿平台，同时对房东和租客开放。房东可以在平台上发布房源信息，租客通过搜索选取自己心仪的房源(<strong><em>list</em></strong>)进行预定。同时，房东也可以决定是否接受租客的预定。Airbnb的目标在于，如何利用几种平台上有效的用户和租客行为<strong>点击</strong>(click)，<strong>预定</strong>(booking)，<strong>拒绝预定</strong>(reject)，学习到有效的embedding，从而在搜索中<strong>实时</strong>向用户推荐更符合他们<strong>偏好</strong>（preference）的房源信息。</p>
<p>Airbnb把用户偏好分为了<strong>短期兴趣</strong>(short-term interest)和<strong>长期兴趣</strong>(long-term interest)。短期兴趣捕捉用户实时搜索浏览的房源特征，长期兴趣则更多从用户的预定历史中挖掘更普遍的兴趣偏好，例如一个当前在浏览巴黎房源信息的租客，他过去在洛杉矶预定的房源特征同样可以提供一种偏好信息(cross-market similarities)。</p>
<h4 id="短期兴趣">短期兴趣</h4>
<p>在word2vec的部分我们说过，将其应用到其他场景的一个关键就是如何生成有效的“上下文信息”。Airbnb结合自身业务场景的embedding策略可总结为以下三点：</p>
<ul>
<li>每个租客30min内点击的list为一个click session，即由list组成的序列。序列对应词嵌入模型中的“句子”，每个list对应句子中的“词语”，点击信息对应“上下文信息”；</li>
<li>把每个租客的历史预定作为booked list，假设booked list始终与central list有关，即始终出现在当前训练窗口；</li>
<li>租客一段时间内的点击行为会有一个特殊的location偏好。例如一个准备去巴黎的租客不会浏览伦敦的房源信息。为了缓解负采样中大量负样本极有可能来源于不同地区这一问题，在目标函数引入与当前central list具有相同位置信息的负样本集。</li>
</ul>
<p>有了“上下文”序列后，我们直接看模型的目标函数，可以看出，训练是基于负采样的skip-gram。其中，l为当前的central list，对应中心词<span class="math inline">\(w_c\)</span>；c为context list，对应背景词<span class="math inline">\(w_o\)</span>。<span class="math inline">\(D_p\)</span>为正样本集，<span class="math inline">\(D_n\)</span>为负样本集。<span class="math inline">\(D_{m_n}\)</span> 为采样自与当前click list相同location的负样本集。</p>
<img src="/2021/06/21/graph-embedding-0/airbnb_objfunc.png" class="">
<!-- $$
argmax_{\theta} \sum_{(l, c)\in D_p} log \frac {1}{1+e^{-v^{'}_cv_l}} +
\sum_{(l, c)\in D_n} log \frac {1}{1+e^{v^{'}_cv_l}} +
log \frac {1}{1+e^{-v^{'}_{l_b}v_l}} +
\sum_{(l, m_n)\in D_{m_n}} log \frac {1}{1+e^{v^{'}_{m_n}v_l}}
$$ -->
<p>除此之外，Airbnb还指出了如何在短期兴趣的学习中解决冷启动的问题。如果有新的list出现，则在附近(within a 10 miles radius)寻找三个相同类型，价格接近的lists，将它们embedding的平均作为new list的embedding。</p>
<h4 id="长期兴趣">长期兴趣</h4>
<p>到这里，Airbnb通过click session成功学习到了short-term interest的list embedding。然而，对于长期兴趣(booking history)却很难用book session去训练，原因是我们没有足够的数据，这在训练过程中会带来严重的数据稀疏问题。因此，Airbnb不再直接学习user embeddings和list embeddings，而是转而学习<strong><em>user_type embeddings</em></strong>和<strong><em>list_type embeddings</em></strong>。这相当于提前进行了一个类型聚类，可以一定程度上解决数据稀疏和冷启动问题。</p>
<p>以论文中的表格为例，无论是用户还是房源都会存在基础属性信息，Airbnb把这些基础信息组合起来表示user_type和list_type。例如一个美国的房源，房源基础信息为<em>lt_1</em>，每晚价格60美金为<em>pn3</em>，则该房源的<span class="math inline">\(listing\_type = US\_lt_1\_pn3\)</span>。user_type同理。Airbnb把一个book session定义为同一user的预定历史，每一项是在预定时间的user_type和list_type的组合--&gt;(<span class="math inline">\(u_{type_1}l_{type_1}, ..., u_{type_M}l_{type_M}\)</span>)，因为同一user的user_type，相同list的list_type都可能会随时间变化而有所不同。</p>
<p>接下来，基于book session所提供的“上下文信息”，继续延用基于负采样的skip-gram训练模式。如下图所示，图中红色部分为房主的reject信息，与上述负采样信息类似，旨在用这部分信息降低租客被拒绝的概率。另外需要注意的一点是，论文中想要在同一个向量空间中学习user_type和list_type的embeddings，因此在book session的序列中，实际上是把它们当成了完全相同的节点。我的理解是，与短期点击行为只需返回与当前浏览的list相似的房源信息不同，长期兴趣更想计算出的是当前user_type与不同list_type之间的相似性。因此，要保证向量cosine相似性的计算有意义，Airbnb选择了在同一向量空间中学习二者的embedding。</p>
<img src="/2021/06/21/graph-embedding-0/long_term_train.png" class="">
<hr>
<p>Airbnb将点击行为序列直接用于词嵌入模型的输入。然而，这样的训练数据可能是不充分的。例如，本来相似的item在独立的用户行为序列中可能并不存在共现，用户1可能浏览了ABC，用户2可能浏览了CD，在训练数据中并不存在的ABCD序列，这可能会使模型不能学习到A与D以及B与D的这部分相似性信息。</p>
<hr>
<p>接下来，我们将词向量的训练原理类比到graph embedding中。我们再来看图嵌入的定义，与自然语言中词语之间的上下文信息相对应的，图中的节点通过不同类型的连接关系形成了邻域，这同样为我们提供了一种“上下文信息”，如何在节点特征中融入邻域信息同样是图嵌入的关键。</p>
<h3 id="deepwalk">deepWalk</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1403.6652.pdf">论文原文</a></p>
<p>与词嵌入不同的是，图结构中节点的邻域是一种二维信息，而非句子序列类的一维信息。那么，如何让我们定义的节点邻域有意义，且怎么处理这种二维信息呢？ Perozzi等人在2014年提出的<strong>deepWalk</strong>模型，成功架起了从word2vec到Graph Embedding的桥梁。deepWalk选择了<strong>随机游走</strong>（<strong>Random Walk</strong>）的方法把二维邻接信息转化成了节点序列。文中指出，随机游走的节点分布规律与NLP中句子序列在语料库中出现的规律相似，都符合幂律分布(power-law distribution)的特征，这使得deepWalk合理地将词向量的训练模式用到了图的表示学习中。</p>
<img src="/2021/06/21/graph-embedding-0/deepwalk.png" class="">
<p>如上图所示，在给定游走策略为<span class="math inline">\(R\)</span>，游走步长<span class="math inline">\(k\)</span>的前提下，以节点<span class="math inline">\(u\)</span>为起始点进行随机游走。由于游走过程基于图中的连接关系，随机游走的每一条序列都包含了起始节点半径为<span class="math inline">\(k\)</span>的邻域 <span class="math inline">\(N_{R(u)}\)</span> 信息。游走序列和自然语言中的句子意义不谋而合。此外，使用随机游走还有以下几个优点：</p>
<ul>
<li>缓解数据稀疏性。在图中连接较少的情况下，随机游走可以生成大量序列，补充了训练样本。</li>
<li>并行化。随机游走是局部信息采样，因此对于大网络可以同时在不同的顶点进行游走，减少采样时间。</li>
<li>动态适应。网络局部的变化只会对部分随机游走路径产生影响，因此不需要每一次网络的更新都重新进行随机游走采样。</li>
</ul>
<p>给定顶点<span class="math inline">\(u\)</span>进行随机游走，deepWalk通过最大化出现在窗口内其他节点<span class="math inline">\(v\)</span>的概率来学习<span class="math inline">\(u\)</span>的表示。顶点<span class="math inline">\(u\)</span>对应了skip-gram中的中心词<span class="math inline">\(w_c\)</span>，节点<span class="math inline">\(v\)</span>即为背景词<span class="math inline">\(w_o\)</span>。deepWalk的目标为最大化以下联合概率分布：</p>
<p><span class="math display">\[
argmax \prod_{u \in V}\prod_{v \in N_{R(u)}} P(v|e_u)
\]</span></p>
<p>同时，为了避免计算 <span class="math inline">\(P(v|e_u)\)</span> 时对整个顶点集合大小的遍历，deepWalk的原论文中采用了层序softmax的优化方法，通过树的结构性质把原本<span class="math inline">\(O(|V|)\)</span> 的计算代价优化为了<span class="math inline">\(O(log|V|)\)</span>。在实际应用中，也可以采用负采样进行优化。</p>
<!-- 由于$P(e_v|e_u)$ 计算代价过大，deepWalk采用层序softmax对训练过程进行优化。deepWalk用所有节点构建了一棵树，每个节点是这棵树的叶子节点，因此到每个节点都有一条唯一的路径。层序softmax把计算$P(e_v|e_u)$ 的问题转化成最大化层序遍历的过程中最大化某一条路径的方式(maximize the probability of a specific path in the hierarchy)。由于树的结构性质，层序softmax把原本$O(|V|)$ 的计算代价优化为了$O(log|V|)$。 -->
<!-- 如果到节点$u_k$的路径为( $b_0$, $b_1$, ..., $b_{log|V|}$) ($b_0 = root$, $b_{log|V|} = u_k$)，那么

$$
\begin{matrix}
P(e_v|e_u) = \prod_{l=1}^{log|V|}P(b_l|e_u)\\
P(b_l|e_u) = \frac {1} {1+e^{-e_u \cdot {e_{b_l}}}}
\end{matrix}
$$ -->
<!-- $$
\begin{matrix}
P(u_k|\phi(v_j)) = \prod_{l=1}^{log|V|}P(b_l|\phi (v_j))\\
P(b_l|\phi (v_j)) = \frac {1} {1+e^{-\phi (v_j) \cdot {}}}
\end{matrix}
$$ -->
<!-- 其中，$e_{b_l}$ 代表了树中节点$b_l$的父节点向量。由于树的结构性质，层序softmax把原本$O(|V|)$ 的计算代价建为了$O(log|V|)$。 -->
<p><strong><em>P.S.</em></strong> 理论上deepWalk适用于边无权重的图，但在实际应用的过程中，可以把边的权重和随机游走的转移概率结合在一起，所以也比较灵活。这也是在deepWalk的基础上产出的诸多衍生模型的做法。</p>
<h3 id="node2vec">node2vec</h3>
<p><a target="_blank" rel="noopener" href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf">论文原文</a></p>
<p>可以看出，游走策略<span class="math inline">\(R\)</span>实际上起到了定义节点邻域的作用，它决定了生成的序列可能包含什么样的信息，也就决定了embedding中包含了什么信息。在deepWalk中，均匀的随机游走策略无法捕捉到相隔较远的两个节点之间的相似性，因此，另一算法node2vec对deepWalk的游走策略进行了改进。</p>
<p>node2vec认为，图结构主要表征了两种类型的相似性，一种为<strong>同质性</strong>，一种为<strong>结构性</strong>。同质性认为，图中属于同一个簇或社群的节点应该相似；而结构性则认为，在不同簇或社群中角色相同的节点应该相似。node2vec因此采用了更加灵活的方式来生成邻域<span class="math inline">\(N_{R(u)}\)</span>，分别是对图的<strong>BFS</strong>（深度优先搜索）和<strong>DFS</strong>（广度优先搜索），使得模型可以捕捉这两种不同的相似性信息。</p>
<ul>
<li><p>BFS: 局部信息(local microscopic view)，更能捕捉结构性</p></li>
<li><p>DFS: 全局信息(global macroscopic view)，更能捕捉同质性</p></li>
</ul>
<p>因此，node2vec给定两个超参数用以指导随机游走：</p>
<ul>
<li><p>return parameter \<span class="math inline">\(p\)</span>: 控制返回到上一个节点</p></li>
<li><p>in-out parameter \<span class="math inline">\(q\)</span>: 控制是否远离当前节点（DFS）或靠近当前节点（BFS）</p></li>
</ul>
<img src="/2021/06/21/graph-embedding-0/node2vec.png" class="">
<p>这两个超参数控制随机游走的转移概率。<span class="math inline">\(1/q\)</span>控制着更远一步的概率。<span class="math inline">\(q\)</span>越小，随机游走生成的序列越接近于DFS遍历，序列所捕获的邻域信息越接倾向于全局信息；<span class="math inline">\(1/p\)</span>控制着返回上一个节点的概率。<span class="math inline">\(p\)</span>越小，随机游走生成的序列越接近于BFS遍历，序列所捕获的邻域信息越接倾向于局部信息。前者更适用于捕捉community之间的相似性，后者更适合于捕捉图中的层次化/结构化信息。以社交网络为例，前者更适合捕捉社群信息，如给用户推荐相似好友。后者则更适合捕捉用户在社交网络中的角色信息，如果两位用户好友都很多，则他们都位于一个社群的中心位置，在后者的游走策略之下，他们在嵌入空间中的向量就会比较相似，如下图所示。</p>
<img src="/2021/06/21/graph-embedding-0/node2vec_res.png" class="">
<p>对于文中所提的同质性和结构性，有个问题困扰了我很久。因为从直觉上来说，DFS策略能走到更远的节点，距离很远的两个簇的中心更容易出现在DFS的游走序列里，因此更能捕捉全局结构信息；而BFS策略更多的是在节点周围游走，相聚很远的两个簇中心，即使结构相似，也不太可能频繁出现在BFS的序列里。那么<strong><em>为什么是BFS更能捕捉结构信息，DFS反而是捕捉同质信息</em></strong>？这个问题我查了很久也没有得到一个很有说服力的结论，可能最终还是要看实验结果。希望有经验的同学可以解惑。</p>
<p><strong><em>P.S.</em></strong> 对deepWalk和node2vec的讲解，强推斯坦福大学CS224W课程(Machine Learning with Graph)中Graph Representation Learning这一节。<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=YrhBZUtgG4E&amp;t=2983s">课程链接点我</a>, <a target="_blank" rel="noopener" href="http://snap.stanford.edu/class/cs224w-2018/handouts/09-node2vec.pdf">slides点我</a>。</p>
<hr>
<p>deepWalk给很多应用模型提供了思路，有许多工业界模型都在它的基础上结合业务场景进行了改进，在实际应用上取得了不错的效果。</p>
<h3 id="工业界应用-eges---billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba">工业界应用: EGES - Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02349.pdf">论文原文</a></p>
<p>阿里的这篇论文重点解决推荐系统中的召回部分。其主要亮点在于引入side information对冷启动问题的解决。</p>
<p>在学习item_embeddings时，和Airbnb类似，阿里同样使用了基于用户session生成的序列作为词嵌入模型的输入。与Airbnb不同的是，阿里没有直接使用用户的点击序列，而是用不同用户的点击行为先构造一个有向图。如下图所示，用户U1在一个时间窗口内先后点击了D和A，则建立一条由D指向A的边。如果有另一个用户也有相同顺序的点击行为，那么D和A之间边的权重会加1。在基于session构造了item graph后，利用deepWalk的思想，先进行随机游走生成序列，再利用词嵌入模型进行训练。</p>
<img src="/2021/06/21/graph-embedding-0/eges_frm.png" class="">
<p>然而，上述基于用户行为的建图及游走过程无法解决冷启动问题，对于用户行为很少的item并不能输出满意的embedding结果。因此，这篇论文在训练阶段引入了item的side information作为可学习的特征，在用户行为很少的情况下期望利用这部分特征来对item进行有效的表示。如下图所示，<em>SI 0</em> 为item本身的特征，即上述session中学到的embedding。对于新item这部分可以初始化为0向量。<em>SI 1,... SI n</em> 为不同类型的side information，</p>
<img src="/2021/06/21/graph-embedding-0/side_info.png" class="">
<p>在隐含层对代表不同信息的embedding进行了聚合。论文中给出了两种聚合方法，一种是普通的average pooling，另一种则是加权平均。后者在特征融合阶段又增加一个可训练的权重矩阵，考虑不同信息对于最后得到的item_embedding的重要程度。</p>
<!-- 在论文的future work里，也提到了这一部分可以应用attention机制。 -->
<!-- ***在NLP中，一个句子中词的存在天然的语义关系，因此word2vec所学习到的这部分序列信息是有意义的。然而在推荐系统中，用户的点击行为之间可能并不存在很强的序列关系。？？？？*** -->
<hr>
<p>deepWalk等基于节点进行随机游走的方法，在涉及信息更复杂的异构图时，表现可能很差。在异构图中，节点和边共同构成完整的语义信息(semantics)，基于节点的随机游走会割裂这种关系。以下图学术网络这个典型的异构图为例，网络中存在四种节点类型，不同节点之间的边包含不同的语义信息。例如，author节点<span class="math inline">\(a_1\)</span>和paper节点<span class="math inline">\(p_1\)</span>的边代表<span class="math inline">\(a_1\)</span>是<span class="math inline">\(p_1\)</span>的作者，而<span class="math inline">\(a_1\)</span>和<span class="math inline">\(a_2\)</span>之间的边则代表了两人是coauthor的关系。由于author类型的节点数比较多，基于节点的随机游走生成的大部分序列中都会包含author节点。在限定游走的步数的前提下，数目比较少的节点类型如org出现的概率会很小。在异构图中特定语义信息约束下，会导致这部分节点及边信息训练数据不够，因此无法生成令人满意的embedding；此外，在语义关系明确的异构图中，deepWalk可能会产生信息不明确的序列。例如，以节点<span class="math inline">\(a_1\)</span>为顶点进行步长为5的随机游走时，一个可能的序列是{<span class="math inline">\(a_1, a_2, a_3, a_4, CMU\)</span>}。然而，<span class="math inline">\(a_1\)</span>和<span class="math inline">\(CMU\)</span>并没有明确的关系，这样可能会在<span class="math inline">\(a_1\)</span>中错误包含并不那么重要的信息，影响embedding的质量。</p>
<img src="/2021/06/21/graph-embedding-0/academic_net.png" class="">
<p>在异构图中，存在一种元路径的定义。同样以学术网络为例，APA和APVPA是两种metapath，前者代表了两个作者为一篇论文的coauthor，后者代表两个不同作者在同一个venue发表过论文。可以看出，元路径实际代表了不同类型节点或者相同类型节点之间存在的某种特定关系。</p>
<h3 id="metapath2vec">metapath2vec</h3>
<p><a target="_blank" rel="noopener" href="https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf">论文原文</a></p>
<p>metapath2vec基于元路径对deepWalk的模式进行了改进。metapath2vec的基本架构和deepWalk类似，即随机游走生成序列加上skip gram训练模型，区别在于：</p>
<ul>
<li><p>metapath2vec采用了基于元路径的随机游走来生成序列。提前定义的元路径作为一种先验信息在指导着随机游走，使得生成的序列都是符合图中原本语义的，这很好地捕捉了图中的异构信息。</p></li>
<li><p>metapath2vec采用了异构skip gram模型。如下图公式阴影部分所示，与普通的skip gram的区别在于定义了不同类型节点的异构邻域。</p></li>
</ul>
<img src="/2021/06/21/graph-embedding-0/hete_skip_gram.png" class="">
<p>我在查阅资料的过程中，在xx上看到了xx团队发的一篇graph embedding结合具体业务场景模型落地的<a href="">文章</a>。xx团队在实验过程中，构造了不同类型的标签节点将用户行为生成的同构图扩展为了异构图，再通过metapath2vec的方法基于定义的元路径进行随机游走。首先，标签节点本身可以作为item的一种side information，基于元路径的随机游走相当于学习了这部分属性信息；其次，构造如“作者+二级类目”这种强属性的标签节点，可以使生成的序列有更强的倾向性。</p>
<h3 id="hin2vec">HIN2Vec</h3>
<p><a target="_blank" rel="noopener" href="https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiz-uXo0p7xAhVyqksFHSo2CYMQFjABegQIBRAD&amp;url=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3132847.3132953&amp;usg=AOvVaw0QleQ3h0R4RlhDkFTqvbfH">论文原文</a></p>
<p>除此之外，HIN2Vec中提出了另外一种可以保持异构网络语义信息的方法：以图中多种关系类型为label训练一个多分类模型，来预测任意两个节点之间可能存在的关系。与metapath2vec类似，HIN2Vec也默认元路径中包含了异构图中的多种关系信息，因此利用这部分信息指导embedding的生成。不同的是，HIN2Vec没有在随机游走的过程中以元路径作为指导，而是依然以节点进行随机游走生成节点序列，转而将metapath定义为图中的关系类型集合，作为label训练多分类器。HIN2Vec的最初设想是，给定节点对 <span class="math inline">\(&lt;x, y&gt;\)</span> ，预测节点之间的存在系集合<span class="math inline">\(R\)</span>中的哪几种关系。然而，找寻所有可能的关系是不现实的。因此。HIN2Vec将多分类问题简化为二分类问题：</p>
<img src="/2021/06/21/graph-embedding-0/hin2vec.png" class="">
<p>以上图为例，关系<span class="math inline">\(r\)</span>取自预先定义的由元路径组成的关系集合<span class="math inline">\(R\)</span>。HIN2Vec通过判断节点对<span class="math inline">\(&lt;x, y&gt;\)</span> 之间是否存在关系<span class="math inline">\(r\)</span>来训练一个二分类模型，将异构网络中的信息集成在分类器的权重<span class="math inline">\(W\)</span>中。</p>
<!-- question: metapath真的可以包含异构图的关系信息吗？ -->
<hr>
<h3 id="动态图嵌入">动态图嵌入</h3>
<p>值得一提的是，虽然上述方法的有效性已经得到了大量学术实验的验证，但在实际应用中，仍然有关键的一点是我们关注的，那就是“<em>如果有新加入的节点，删除的节点，新加入的边等问题要怎么处理呢？</em>”这一<strong>动态性图嵌入</strong>(<strong>Dynamic Network Embedding</strong>)的问题。在xx团队的一篇十分精彩的<a href="">博客</a>里，提到了在支付的场景下，需要对用户embedding进行增量更新。他们的解决思路是，针对发生变化的子图部分进行随机游走，生成新的游走序列来表征图中的局部更新信息，从而以这较小的代价来更新模型。然而，首先，我们很难圈定节点更新时的影响范围；第二，局部更新总会促成全局更新，我们很难找到一个确切的时间节点对模型进行全量更新；第三，在业务应用中对embedding的要求更高，要求更新快速有效，同时具备高稳定性。针对这些问题，xx团队的思路是，可以通过时序信息将用户节点的“短期波动与长期稳定行为解耦”，在下一个时间窗口通过用户相对静止的embedding与交互节点的embedding信号直接进行更新。这是站在时序的角度，从不同时间节点图的特征中剥离出了稳定信息和波动信息，从而更有针对性地进行embedding的增量更新。</p>
<p>另一篇论文<a target="_blank" rel="noopener" href="https://www.ijcai.org/Proceedings/2018/0288.pdf">Dynamic Network Embedding - An Extended Approach for Skip-gram based Network Emebdding</a>中提出了另一种解决方案。文中对图嵌入方法常用的目标函数进行了数学上的分解(<em>decomposable objective</em>)，使原本的目标函数可以更新局部节点，而不是更新全部。接下来，作者又计算出了每次图结构发生变化时被影响最大的是哪些节点，以便有针对性地进行更新。分解过的目标函数可以很快计算出新加入的节点的representation，又可以对影响最大的节点进行局部更新。</p>
<h3 id="结语">结语</h3>
<p>metapath作为一种先验知识，可以很好地指导embedding的训练过程。然而，无论是顶点的随机游走，还是metapath指导下的随机游走，其训练都是基于word2vec，对于序列中的节点只有同一时间窗口的共现概念，不会考虑其顺序信息，因此不会再考虑节点之间是一阶邻居还是二阶邻居。可以说，展开的序列信息已经一定程度上失去了图数据本身的结构信息。那么，有没有一种方式可以直接在图的拓扑结构上聚合特征，而不是先把二维结构展开成一维序列？这种方式和Graph Embedding相比又有什么优缺点呢？下一篇会介绍基于<strong>图神经网络</strong>(<strong>Graph Neural Network</strong>)的图嵌入方法。</p>
<!-- 2017年，Kipf和Welling在第一代基于xx的GCN[]()、第二代基于xx的GCN[]()的基础上，提出了第三代GCN--xx。相比如第二代GCN，这篇论文中xx，GCN也因此而大火。

论文原文的数学部分确实晦涩难懂。我学术水平尚浅，有关谱聚类、傅立叶变换无法做到深入浅出的解释。我们只站在宏观的角度来看一下GCN的思想： -->

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/06/16/Graph-Embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/06/16/Graph-Embedding/" itemprop="url">Graph Embedding</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-06-16T13:31:37+03:00">
                2021-06-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/" itemprop="url" rel="index">
                    <span itemprop="name">paper_reviews</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.6.16</i> </font>
</p>
<hr>
<p>图结构(Graph)广泛存在于各种应用场景中。如在社交媒体中，用户之间的关注关系构成了一个大的社交网络(Social Networks)；又如推荐系统(Recommender Systems)中，用户对商品的关注、评分等行为把可以抽象成用户和商品的关系图。然而，现实中的图结构大多都很复杂，如一个社交媒体中可能有几十亿用户、几千万条边，同时，图中的连接关系可能包含复杂的信息，如一个用户对商品的反馈可能是正面的，也可能是负面的。这因此给图结构数据的建模带来了困难。</p>
<p>图嵌入(Graph Embedding)为此提供了很好的解决问题的方法。嵌入(Embedding)的思想是，我们希望可以把节点的特征信息嵌入到一个低维空间中，使得有连接的节点在低维空间中的向量彼此靠近，以此达到降维和图表示学习的目的。如果我们以数学语言描述图嵌入问题：</p>
<p>给定图<span class="math inline">\(G=(V, E)\)</span>，其中<span class="math inline">\(V\)</span>代表节点(<strong>Vertex</strong>)集合，<span class="math inline">\(E\)</span>代表图中边(<strong>Edge</strong>)集合。定义变量<span class="math inline">\(Type_v\)</span>，<span class="math inline">\(Type_e\)</span>，分别代表图中节点类型集合和边类型集合。对于图<span class="math inline">\(G\)</span>，当 <span class="math inline">\(|Type_v| + |Type_e| = 2\)</span>时，我们称<span class="math inline">\(G\)</span>为同构图(Homogenous Network)，即节点类型和边类型只有一种的图，比如社交网络；对于图<span class="math inline">\(G\)</span>，当 <span class="math inline">\(|Type_v| + |Type_e| \geq 3\)</span> 时，我们称<span class="math inline">\(G\)</span>为异构图(Heterogenous Network)，即节点类型边类型不止有一种的图，比如用户和商品的关系图。</p>
<p>对于图中节点<span class="math inline">\(u \in V\)</span> ，定义节点<span class="math inline">\(u\)</span>的<strong>邻域</strong>(<strong>neighborhood</strong>)为<span class="math inline">\(N_u\)</span>，我们希望找到一种方法<span class="math inline">\(f:u\rightarrow R^{d \times |V|}\)</span>，将节点特征映射到一个低维度的向量空间中，空间中的向量 <span class="math inline">\(e_u \in R^{d \times |V|}\)</span> 能够很好地聚合<span class="math inline">\(u\)</span>的邻域信息，使得 <span class="math inline">\(v \in N_u\)</span>的向量 <span class="math inline">\(e_v \in R^{d \times |V|}\)</span> 在嵌入空间中与<span class="math inline">\(e_u\)</span>尽可能接近。如下图所示，我们把 <span class="math inline">\(R^{d \times |V|}\)</span> 称为嵌入空间(Embedding Space)，<span class="math inline">\(f\)</span>就是我们希望找到的嵌入方法。</p>
<img src="/2021/06/16/Graph-Embedding/graph_emb.png" class="">
<p>本文介绍了常见的Graph Embedding算法，重点放在词嵌入模型出现之后以此为基础提出的图嵌入方法及其衍生模型，并分析了工业界如何针对自身具体的业务场景在实际落地的过程中对模型进行了改进。</p>
<p>本文目录如下：</p>
<h2 id="matrix-factorization">Matrix Factorization</h2>
<p>矩阵分解的理论基础来源于数学中的伪奇异值分解(Singular Value Deposition)。之所以是奇异值分解而不是特征值分解，是因为在实际场景中我们要分解的矩阵不一定是方阵。它在推荐系统中有着广泛的应用，对推荐系统熟悉的同学一定记得2006年netflix的那场著名的比赛，矩阵分解在那场比赛中大放异彩。</p>
<p>以user-item-rating这一矩阵为例，矩阵分解将大矩阵<span class="math inline">\(R:m \times n\)</span> 分解为两个小矩阵<span class="math inline">\(U:m \times k\)</span>，<span class="math inline">\(I:k \times n\)</span> 的乘积：</p>
<p><span class="math display">\[R = U\times I\]</span></p>
<p>这其中的k维向量，代表隐因子（Latent Factor）向量。矩阵分解认为，隐因子一定程度上代表了user和item的某种属性。在user-item-rating这一特定的矩阵中，这两个子矩阵可以分别代表user的偏好程度和item的成分属性。以电影评分矩阵为例，隐因子的<span class="math inline">\(K\)</span>维可能代表<span class="math inline">\(K\)</span>种电影的成分类型，如悬疑成分、科幻成分等等，那么分解得到的用户矩阵<span class="math inline">\(U\)</span>的第k维<span class="math inline">\(U_k\)</span>则代表了用户对于这一成分的偏好程度， 电影矩阵<span class="math inline">\(I\)</span>的第k维<span class="math inline">\(I_k\)</span>则代表了电影内这一成分的占比。矩阵分解认为，用户对不同成分的偏好与电影中不同成分的占比的乘积代表了用户对电影的评分。</p>
<img src="/2021/06/16/Graph-Embedding/mf.png" class="">
<p>对于user-item-rating矩阵而言，评分的预测实际上是一个补全矩阵的过程。原本的矩阵中，有一部分有评分，而有一部分没有。因此，矩阵分解的目标是使得最终得到的两个小矩阵的乘积可以很好地补全大矩阵中没有评分的未知项，如何获得这两个最优的小矩阵即如何学习到满意的可以代表user、item属性的嵌入向量矩阵。在应用中，我们可以以评分的部分与预测得到的评分误差最小为基础构造损失函数，再利用随机梯度下降等方法进行模型训练。然而，矩阵分解的方法在大规模数据集下训练会很慢。</p>
<p><strong><em>P.S.</em></strong> 对于矩阵分解的详细介绍这里推荐xx的三篇km文章<a href="">1</a>，<a href="">2</a>，<a href="">3</a>，从原理到应用都介绍得十分精彩。</p>
<!-- 再推荐一篇[博客]()，介绍十分详细具体。 -->
<h2 id="embeddings">embeddings</h2>
<p>word2vec的出现使得表示学习进入了“万物皆可embedding”的时代，以embedding为基础的图嵌入的方法也同样是受NLP中的词向量模型启发。对NLP来说，在把词数据送入到机器学习模型之前，最简单直观的编码方式是one hot编码。即每一个向量用一个大小和语料库大小的向量表示，当前时刻该词语所在的位置置为1，剩余位置置为0。这样虽然也达到了唯一标识的目的，却失去了词语之间的关系信息。自然语言中，词语与词语之间构成句子、段落，使得不同的词语组合存在一种天然的语义联系——也就是上下文信息，word2vec正是希望通过捕捉这种上下文信息来找寻词向量的嵌入空间。在把这一思路运用到Graph Embedding之前，我们先介绍一下word2vec的原理。</p>
<!-- 词嵌入的出现使得表示学习进入了“万物皆可embedding”的时代，以embedding为基础的图嵌入的方法也同样是受NLP中的词向量模型启发。对NLP来说，如果用one-hot这种local representation来表示语料库中的每个词语，就会失去词语之间的关系信息。自然语言中，词语与词语之间构成句子、段落，使得不同的词语组合存在一种天然的语义联系——也就是上下文信息。那么，如何找寻这份联系？word2vec应运而生，它提供了一种训练distributed representation词向量的模式。word2vec认为句子之间的上下文信息就是one-hot编码无法捕捉到的信息，也是我们找寻嵌入空间的关键所在。在把这一思路运用到Graph Embedding之前，我们先介绍一下word2vec的原理。 -->
<h3 id="理论基础word2vec">理论基础：word2vec</h3>
<p><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">论文原文</a></p>
<p>在自然语言中，词语构成句子，我们首先给句子设定一个固定大小的窗口(<em>Window</em>)，只关注在这个窗口内的词语。如句子“拜仁是欧冠冠军”由词语“拜仁，是，欧冠，冠军”组成。我们以“欧冠”这个词语为中心，考虑窗口大小为2的词，那么“拜仁，是，冠军”这几个词语就是“欧冠”的上下文。我们把“欧冠”定义为<strong>中心词</strong>(<em>Center Word</em>)，把上下文词语定义为<strong>背景词</strong>(<em>Context Word</em>)。在word2vec中，每个词语都有两个向量，即中心词向量和背景词向量，取决于当前时刻该词语的角色。word2vec提供了两种方法，分别是以中心词向量生成背景词向量概率分布的skip-gram，和以背景词向量生成中心词向量概率分布的c-bow。以skip-gram为例，word2vec希望通过最大化背景词出现的概率来对中心词进行有效的表示。</p>
<h4 id="skip-gram">skip-gram</h4>
<p>根据最大似然估计( <strong>M</strong>aximum <strong>L</strong>ikelihood <strong>E</strong>stimation )的思想，skip-gram的目标是最大化当前中心词的所有背景词向量的联合概率分布，即：</p>
<p><span class="math display">\[argmax  \frac{1}{T}\prod_{t=1}^T \prod_{-m\leq j\leq m}log P(w^{t+j}_o|w^t_c)\]</span></p>
<p>其中，<span class="math inline">\(T\)</span>代表词库大小，<span class="math inline">\(w_c\)</span>为中心词，<span class="math inline">\(w_o\)</span>为背景词，m为窗口大小。<span class="math inline">\(P(w_o|w_c)\)</span> 代表背景词<span class="math inline">\(w_o\)</span>的概率分布，<span class="math inline">\(u\)</span>代表背景词向量，<span class="math inline">\(v\)</span>代表中心词向量，定义为一个softmax形式的函数：</p>
<p><span class="math display">\[P(w_o|w_c) = \frac{exp(u_o^Tv_c)}{\sum_{i\in T}exp(u_i^Tv_c)}\]</span></p>
<p>可以看到，在求解背景词的概率分布时，由于背景词可能是词典中的任意一个词，导致我们要对整个词典大小的词进行遍历，这样的计算代价显然是不现实的。word2vec中提出了两种优化训练的方法，分别是<strong>负采样</strong>（<strong>negative sampling</strong>）和层序softmax。</p>
<h4 id="负采样">负采样</h4>
<p>负采样首先基于两个假设：</p>
<ul>
<li>中心词<span class="math inline">\(w_c\)</span>和背景词<span class="math inline">\(w_o\)</span>同时出现在一个训练窗口。</li>
<li>中心词<span class="math inline">\(w_c\)</span>和第k个噪声词<span class="math inline">\(w_k\)</span>不同时出现在一个训练窗口。</li>
</ul>
<p>以skip-gram为例，中心词<span class="math inline">\(w_c\)</span>和背景词<span class="math inline">\(w_o\)</span>同时出现在当前训练窗口的概率为：</p>
<p><span class="math display">\[
P(D=1|w_c, w_o) = \sigma (u_o^T \cdot v_c)
\]</span></p>
<p>同理，中心词<span class="math inline">\(w_c\)</span>和噪声词<span class="math inline">\(w_k\)</span>没有同时出现在当前训练窗口的概率为：</p>
<p><span class="math display">\[
P(D=0|w_c, w_k) = 1 - \sigma (u_k^T \cdot v_c)
\]</span></p>
<p>因此，根据MLE的思想，最大化联合概率分布<span class="math inline">\(P(w_o|w_c) = P(D=1|w_c, w_o) \prod_{k=1}^K P(D=0|w_c, w_k)\)</span>，代入上述概率计算公式，激活函数为sigmoid，优化后的损失函数变为： <span class="math display">\[
L = -log \frac {1} {1+e^{-u_o^T \cdot v_c}} - \sum_{k=1, w_k~P(w)}^K log \frac {1} {1+e^{u_k^T \cdot v_c}}
\]</span></p>
<img src="/2021/06/16/Graph-Embedding/skipgram.png" class="">
<p>训练过程不断更新两个权重矩阵<span class="math inline">\(W\)</span>，<span class="math inline">\(W^{&#39;}\)</span>。skip gram训练结束，每个词都会有两个向量分别在这两个权重矩阵里，前者包含中心词向量<span class="math inline">\(W_{word}\)</span>，后者包含背景词向量<span class="math inline">\(W_{context}\)</span>。我们最后保留输入层到隐含层的权重矩阵<span class="math inline">\(W\)</span>为嵌入矩阵，隐含层的维数即为嵌入的维数。</p>
<h4 id="与矩阵分解的可能联系">与矩阵分解的可能联系</h4>
<p>讲到这里，可以看出，word2vec同样输出了两个矩阵 <span class="math inline">\(W_{word}\)</span> 和 <span class="math inline">\(W_{context}\)</span>，那么它和矩阵分解是否存在某种联系呢？实际上，Levy和Goldberg在其论文<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf">Neural Word Embedding as Implicit Matrix Factorization</a>一文中得出结论，认为word2vec实际上是针对语料库的word-context的<strong><em>PMI矩阵</em></strong> <strong><em>(Pointwise Mutual Information, 点间互信息)</em></strong> 的矩阵分解。PMI主要用于计算词语间的语义相似度，统计得到的两个词语在文本中同时出现的概率越大关联度就越高。如果语料库中从未出现word-context对，那么它们之间的PMI值为0或负无穷。</p>
<p>Levy和Goldberg的结论是，word2vec就是对PMI矩阵的隐式分解。skip gram+negative sampling的训练过程能够将词语和其上下文信息嵌入到一起，两个向量的点积可以表示它们同时出现的可能性，也就是PMI值。即便如此，面对大规模语料，word2vec的训练过程比起矩阵分解要简单许多。此外，分解PMI矩阵时也会受限于很多不易处理的缺失值。</p>
<p><strong><em>P.S.</em></strong> 对word2vec的详细原理介绍推荐<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1411.2738.pdf&amp;xid=25657,15700021,15700124,15700149,15700168,15700186,15700191,15700201,15700208&amp;usg=ALkJrhhNCZKc2CO7hRoTrGd6aH2nBc-ZVQ">Xin Rong的论文</a>，以及YouTube的一个<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=C4X0Cb5_FSo&amp;t=858s">讲解视频</a>，对于其数学原理的细节部分介绍的十分详细。</p>
<p>回归正题。如果我们想将词向量的训练方法应用到其他的场景中，那么需要明确的有两点：</p>
<ul>
<li>如何构造有意义的sequence。在词向量的训练模型中，sequence指的是语料库中的所有句子。这些句子提供了词语之间的上下文关系，是学习语义相似性的基础。</li>
<li>如何结合具体应用场景对训练过程采用负采样或层序softmax优化。</li>
</ul>
<hr>
<p>有了这一部分知识之后，我们将词向量的训练原理类比到graph embedding中。我们再来看图嵌入的定义，与自然语言中词语之间的上下文信息相对应的，图中的节点通过不同类型的连接关系形成了邻域，这同样为我们提供了一种“上下文信息”，如何在节点特征中融入邻域信息同样是图嵌入的关键。与词嵌入不同的是，图结构中节点的邻域是一种二维信息，而非句子序列类的一维信息。</p>
<p>那么，如何让我们定义的节点邻域有意义，且怎么处理这种二维信息呢？在word2vec的基础上，deepWalk、node2vec等算法选择了<strong>随机游走</strong>（<strong>Random Walk</strong>）的方法把二维邻接信息转化成了节点序列。如下图所示，在给定游走策略为<span class="math inline">\(R\)</span>的前提下，以节点<span class="math inline">\(u\)</span>为起始点进行随机游走。由于游走过程基于图中的连接关系，可以说随机游走的每一条序列都包含了起始节点的邻域信息。游走序列和自然语言中的句子意义不谋而合。</p>
<img src="/2021/06/16/Graph-Embedding/deepwalk.png" class="">
<h3 id="deepwalk">deepWalk</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1403.6652.pdf">论文原文</a></p>
<p>deepWalk首先给定游走策略<span class="math inline">\(R\)</span>，以节点<span class="math inline">\(u\)</span>为起始点进行随机游走，生成的所有序列中包含的所有节点组成了节点<span class="math inline">\(u\)</span>的邻域<span class="math inline">\(N_{R(u)}\)</span>。对应word2vec的skip-gram算法，<span class="math inline">\(N_{R(u)}\)</span> 中的节点<span class="math inline">\(v\)</span>即为背景词<span class="math inline">\(w_o\)</span>，<span class="math inline">\(u\)</span>即为中心词<span class="math inline">\(w_c\)</span>。如上图所示，<span class="math inline">\(P_R(v|u)\)</span>实际上对应了skip-gram中给定中心词背景词出现的概率。因此，我们就可以写出deepWalk的损失函数:</p>
<p><span class="math display">\[
\begin{matrix}
L = \sum_{u \in V}\sum_{v \in N_{R(u)}} -log(P(e_v|e_u))\\
P(e_v|e_u) = \frac {exp(e_u^T \cdot e_v)} {\sum_{j\in V}(e_u^T \cdot e_j)}
\end{matrix}
\]</span></p>
<p>这个式子几乎和skip-gram如出一辙。同样的，由于<span class="math inline">\(P(e_v|e_u)\)</span> 计算代价过大，deepWalk采用层序softmax对训练过程进行优化。deepWalk用所有节点构建了一棵树，每个节点是这棵树的叶子节点，因此到每个节点都有一条唯一的路径。层序softmax把计算<span class="math inline">\(P(e_v|e_u)\)</span> 的问题转化成最大化层序遍历的过程中最大化某一条路径的方式(maximize the probability of a specific path in the hierarchy)。由于树的结构性质，层序softmax把原本<span class="math inline">\(O(|V|)\)</span> 的计算代价优化为了<span class="math inline">\(O(log|V|)\)</span>。</p>
<!-- 如果到节点$u_k$的路径为( $b_0$, $b_1$, ..., $b_{log|V|}$) ($b_0 = root$, $b_{log|V|} = u_k$)，那么

$$
\begin{matrix}
P(e_v|e_u) = \prod_{l=1}^{log|V|}P(b_l|e_u)\\
P(b_l|e_u) = \frac {1} {1+e^{-e_u \cdot {e_{b_l}}}}
\end{matrix}
$$ -->
<!-- $$
\begin{matrix}
P(u_k|\phi(v_j)) = \prod_{l=1}^{log|V|}P(b_l|\phi (v_j))\\
P(b_l|\phi (v_j)) = \frac {1} {1+e^{-\phi (v_j) \cdot {}}}
\end{matrix}
$$ -->
<!-- 其中，$e_{b_l}$ 代表了树中节点$b_l$的父节点向量。由于树的结构性质，层序softmax把原本$O(|V|)$ 的计算代价建为了$O(log|V|)$。 -->
<p><strong><em>P.S.</em></strong> 理论上deepWalk适用于边无权重的图，但在实际应用的过程中，可以把边的权重和随机游走的转移概率结合在一起，所以也比较灵活。这也是在deepWalk的基础上产出的诸多衍生模型的做法。</p>
<h3 id="node2vec">node2vec</h3>
<p><a target="_blank" rel="noopener" href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf">论文原文</a></p>
<p>可以看出，游走策略<span class="math inline">\(R\)</span>实际上起到了定义节点邻域的作用，它决定了生成的序列可能包含什么样的信息，也就决定了embedding中包含了什么信息。在deepWalk中，均匀的随机游走策略无法捕捉到相隔较远的两个节点之间的相似性，因此，另一算法node2vec对deepWalk的游走策略进行了改进。</p>
<p>node2vec采用了更加灵活的方式来生成邻域<span class="math inline">\(N_{R(u)}\)</span>，使得捕捉到的相似性信息可以在<strong>局部信息</strong>和<strong>全局信息</strong>之间进行一个权衡。我们知道，对图的遍历有两种策略，分别是<strong>BFS</strong>（深度优先搜索）和<strong>DFS</strong>（广度优先搜索）。这两种策略恰恰对应了两种不同的定义节点邻域的方式：</p>
<ul>
<li><p>BFS: 局部信息(local microscopic view)</p></li>
<li><p>DFS: 全局信息(global macroscopic view)</p></li>
</ul>
<p>因此，node2vec给定两个超参数用以指导随机游走：</p>
<ul>
<li><p>return parameter \<span class="math inline">\(p\)</span>: 控制返回到上一个节点</p></li>
<li><p>in-out parameter \<span class="math inline">\(q\)</span>: 控制是否远离当前节点（DFS）或靠近当前节点（BFS）</p></li>
</ul>
<img src="/2021/06/16/Graph-Embedding/node2vec.png" class="">
<p>这两个超参数控制随机游走的转移概率。<span class="math inline">\(1/p\)</span>控制着返回上一个节点的概率。<span class="math inline">\(p\)</span>越小，随机游走生成的序列越接近于BFS遍历，序列所捕获的邻域信息越接倾向于局部信息；<span class="math inline">\(1/q\)</span>控制着更远一步的概率。<span class="math inline">\(q\)</span>越小，随机游走生成的序列越接近于DFS遍历，序列所捕获的邻域信息越接倾向于全局信息。前者更适用于捕捉community之间的相似性，后者更适合于捕捉图中的层次化/结构化信息。以社交网络为例，前者更适合捕捉社群信息，如给用户推荐相似好友。后者则更适合捕捉用户在社交网络中的角色信息，如果两位用户好友都很多，则他们都位于一个社群的中心位置，在后者的游走策略之下，他们在嵌入空间中的向量就会比较相似，如下图所示。</p>
<img src="/2021/06/16/Graph-Embedding/node2vec_res.png" class="">
<p><strong><em>P.S.</em></strong> 对deepWalk和node2vec的讲解，强推斯坦福大学CS224W课程(Machine Learning with Graph)中Graph Representation Learning这一节。<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=YrhBZUtgG4E&amp;t=2983s">课程链接点我</a>, <a target="_blank" rel="noopener" href="http://snap.stanford.edu/class/cs224w-2018/handouts/09-node2vec.pdf">slides点我</a>。</p>
<hr>
<p>deepWalk和node2vec是word2vec在图结构上的一种实现，它们给很多已经在工业界落地的应用模型提供了思路。有许多工业界模型都在它的基础上结合业务场景进行了改进，在实际应用上取得了不错的效果。</p>
<h3 id="衍生模型">衍生模型</h3>
<h4 id="airbnb---real-time-personalization-using-embeddings-for-search-ranking-at-airbnb">Airbnb - Real-time Personalization using Embeddings for Search Ranking at Airbnb</h4>
<p><a target="_blank" rel="noopener" href="https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb">论文原文</a></p>
<p>Airbnb的这篇论文是word2vec结合业务场景改进的一次成功实践。它没有花哨的模型，甚至根本没有用到图结构。选取这篇论文是因为它很好地融入了平台自身的业务特点，并有很多模型在工业界实践的trick，也取得了很好的落地效果，值得仔细阅读和思考。</p>
<p>Airbnb是一个民宿平台，同时对房东和租客开放。房东可以在平台上发布房源信息，租客通过搜索选取自己心仪的房源(<strong><em>list</em></strong>)进行预定。同时，房东也可以决定是否接受租客的预定。Airbnb的目标在于，如何在搜索中<strong>实时</strong>向用户推荐更符合他们<strong>偏好</strong>（preference）的房源信息。论文把这个问题具体化为，如何在embedding的学习过程中融入几种平台上有效的用户和租客行为<strong>点击</strong>(click)，<strong>预定</strong>(booking)，<strong>拒绝预定</strong>(reject)。</p>
<p>Airbnb把用户偏好分为了<strong>短期兴趣</strong>(short-term interest)和<strong>长期兴趣</strong>(long-term interest)。短期兴趣捕捉用户实时搜索浏览的房源特征，长期兴趣则更多从用户的预定历史中挖掘更普遍的兴趣偏好，例如一个当前在浏览巴黎房源信息的租客，他过去在洛杉矶预定的房源特征同样可以提供一种偏好信息(cross-market similarities)。</p>
<!-- 其中，短期兴趣为实时采集一个固定时间窗口内用户点击list的序列(click session)，论文中设定的时间窗口为30min。
Airbnb通过click session生成了list sequences，包含了用户短期内对于list的偏好信息。在训练过程中，Airbnb结合自身业务场景进行了：
 -->
<h5 id="短期兴趣">短期兴趣</h5>
<p>在word2vec的部分我们说过，将其应用到其他场景的一个关键就是如何生成包含有效信息的序列，即对应图结构中的邻域，通过学习这部分有效的“上下文”信息来获取embeddings。Airbnb结合自身业务场景的embedding策略可总结为以下三点：</p>
<ul>
<li>每个租客30min内点击的list为一个click session，即由list组成的序列。序列对应词嵌入模型中的“句子”，每个list对应句子中的“词语”，点击信息对应“上下文信息”；</li>
<li>把每个租客的历史预定作为booked list，假设booked list始终与central list有关；</li>
<li>租客一段时间内的点击行为会有一个特殊的location偏好。例如一个准备去巴黎的租客不会浏览伦敦的房源信息。为了缓解负采样中大量负样本极有可能来源于不同地区这一问题，在目标函数引入当前central list相同位置的负样本集。</li>
</ul>
<p>接下来，套入到word2vec中，以skip-gram + negative sampling为例我们直接给出目标函数。其中，l为当前的central list，对应中心词<span class="math inline">\(w_c\)</span>；c为context list，对应背景词<span class="math inline">\(w_o\)</span>。<span class="math inline">\(D_p\)</span>为正样本集，<span class="math inline">\(D_n\)</span>为负样本集。<span class="math inline">\(D_{m_n}\)</span> 为采样自与当前click list相同location的负样本集。</p>
<img src="/2021/06/16/Graph-Embedding/airbnb_objfunc.png" class="">
<!-- $$
argmax_{\theta} \sum_{(l, c)\in D_p} log \frac {1}{1+e^{-v^{'}_cv_l}} +
\sum_{(l, c)\in D_n} log \frac {1}{1+e^{v^{'}_cv_l}} +
log \frac {1}{1+e^{-v^{'}_{l_b}v_l}} +
\sum_{(l, m_n)\in D_{m_n}} log \frac {1}{1+e^{v^{'}_{m_n}v_l}}
$$ -->
<p>除此之外，Airbnb还指出了如何在短期兴趣的学习中解决冷启动的问题。如果有新的list出现，则在附近(within a 10 miles radius)寻找三个相同类型，价格接近的lists，将它们embedding的平均作为new list的embedding。</p>
<h5 id="长期兴趣">长期兴趣</h5>
<p>到这里，Airbnb通过click session成功学习到了short-term interest的list embedding。然而，对于长期兴趣(booking history)却很难用book session去训练，原因是我们没有足够的数据，这在训练过程中会带来严重的数据稀疏问题。因此，Airbnb不再直接学习user embeddings和list embeddings，而是转而学习<strong><em>user_type embeddings</em></strong>和<strong><em>list_type embeddings</em></strong>。这相当于提前进行了一个类型聚类，可以一定程度上解决冷启动问题。</p>
<p>以论文中的表格为例，无论是用户还是房源都会存在基础属性信息，Airbnb把这些基础信息组合起来表示user_type和list_type。例如一个美国的房源，房源基础信息为bucket 1，每晚价格60美金为bucket 3，则该房源的<span class="math inline">\(listing\_type = US\_lt_1\_pn3\)</span>。user_type同理。因此，Airbnb把book session中的sequence定义为user_type和list_type的组合--&gt;(<span class="math inline">\(u_{type_1}l_{type_1}, ..., u_{type_M}l_{type_M}\)</span>)，同一user的user_type可能会随时间变化而有所不同。</p>
<img src="/2021/06/16/Graph-Embedding/list_type_table.jpeg" class="">
<p>有了sequence之后就继续延用word2vec + negative sampling的训练模式，如下图所示，图中红色部分为房主的reject信息，与上述负采样信息类似，旨在用这部分信息降低租客被拒绝的概率。由于sequence中既有user_type又有list_type，因此训练过程实际上是把它们当成了完全相同的节点，映射到了同一个向量空间里，这在工程角度不失为一种技巧。</p>
<!-- 这里的处理方式我**暂时存疑**。 -->
<img src="/2021/06/16/Graph-Embedding/long_term_train.png" class="">
<hr>
<!-- Airbnb将短期兴趣学习到的list_embedding用于实时的搜索推荐，将长期兴趣学习到的user_type_embedding和list_type_embedding的cosine相似度 -->
<p>Airbnb将点击行为序列直接用于词嵌入模型的输入。然而，这样的训练数据可能是不充分的。例如，本来相似的item在独立的用户行为序列中可能并不存在共现，用户1可能浏览了ABC，用户2可能浏览了CD，在训练数据中并不存在的ABCD序列，这可能会使模型不能学习到A与D以及B与D的这部分相似性信息。</p>
<!-- 此外，对于冷启动的问题，Airbnb -->
<h4 id="eges---billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba">EGES - Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02349.pdf">论文原文</a></p>
<p>阿里的这篇论文主要在于解决推荐中的召回部分。其中亮点在于引入side information对冷启动问题的解决。</p>
<p>在学习item_embeddings时，阿里同样使用了基于用户session生成的序列作为词嵌入模型的输入。与Airbnb不同的是，阿里没有直接使用用户的点击序列，而是用不同用户的点击行为先构造一个有向图。如下图所示，用户U1在一个时间窗口内先后点击了D和A，则建立一条由D指向A的边。如果有另一个用户也有相同顺序的点击行为，那么D和A之间边的权重会加1。在基于session构造了item graph后，利用deepWalk的思想，先进行随机游走生成序列，再利用词嵌入模型进行训练。</p>
<img src="/2021/06/16/Graph-Embedding/eges_frm.png" class="">
<p>然而，上述基于用户行为的建图及游走过程无法解决冷启动问题，对于用户行为很少的item并不能输出满意的embedding结果。因此，这篇论文在训练阶段引入了item的side information作为可学习的特征，在用户行为很少的情况下期望利用这部分特征来对item进行有效的表示。如下图所示，<em>SI 0</em> 为item本身的特征，即上述session中学到的embedding。对于新item这部分可以初始化为0向量。<em>SI 1,... SI n</em> 为不同类型的side information，</p>
<img src="/2021/06/16/Graph-Embedding/side_info.png" class="">
<p>在隐含层对代表不同信息的embedding进行了聚合。论文中给出了两种聚合方法，一种是普通的average pooling，另一种则是加权平均。后者在特征融合阶段又增加一个可训练的权重矩阵，考虑不同信息对于最后得到的item_embedding的重要程度。在论文的future work里，也提到了这一部分可以使用attention机制来做。</p>
<!-- ***在NLP中，一个句子中词的存在天然的语义关系，因此word2vec所学习到的这部分序列信息是有意义的。然而在推荐系统中，用户的点击行为之间可能并不存在很强的序列关系。？？？？*** -->
<hr>
<p>deepWalk等基于节点进行随机游走的方法，在涉及信息更复杂的异构图时，表现可能很差。在异构图中，节点和边共同构成完整的语义信息(semantics)，基于节点的随机游走会割裂这种关系。以下图学术网络这个典型的异构图为例，网络中存在四种节点类型，不同节点之间的边包含不同的语义信息。例如，author节点<span class="math inline">\(a_1\)</span>和paper节点<span class="math inline">\(p_1\)</span>的边代表<span class="math inline">\(a_1\)</span>是<span class="math inline">\(p_1\)</span>的作者，而<span class="math inline">\(a_1\)</span>和<span class="math inline">\(a_2\)</span>之间的边则代表了两人是coauthor的关系。由于author类型的节点数比较多，基于节点的随机游走生成的大部分序列中都会包含author节点。在限定游走的步数的前提下，数目比较少的节点类型如org出现的概率会很小。在异构图中特定语义信息约束下，会导致这部分节点及边信息训练数据不够，因此无法生成令人满意的embedding。</p>
<img src="/2021/06/16/Graph-Embedding/academic_net.png" class="">
<p>在异构图中，存在一种元路径的定义。同样以学术网络为例，APA和APVPA是两种metapath，前者代表了两个作者为一篇论文的coauthor，后者代表两个不同作者在同一个venue发表过论文。可以看出，元路径实际代表了不同类型节点或者相同类型节点之间存在的某种特定关系。</p>
<h3 id="metapath2vec">metapath2vec</h3>
<p><a target="_blank" rel="noopener" href="https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf">论文原文</a></p>
<p>metapath2vec基于元路径对deepWalk的模式进行了改进。metapath2vec的基本架构和deepWalk类似，即随机游走生成序列加上skip gram训练模型，区别在于：</p>
<ul>
<li><p>metapath2vec采用了基于元路径的随机游走来生成序列。提前定义的元路径作为一种先验信息在指导着随机游走，使得生成的序列都是符合图中原本语义的，这很好地捕捉了图中的异构信息。</p></li>
<li><p>metapath2vec采用了异构skip gram模型。如下图公式阴影部分所示，与普通的skip gram的区别在于定义了不同类型节点的异构邻域。</p></li>
</ul>
<img src="/2021/06/16/Graph-Embedding/hete_skip_gram.png" class="">
<p>我在查阅资料的过程中，在xx上看到了xx团队发的一篇graph embedding结合具体业务场景模型落地的<a href="">文章</a>。xx团队在实验过程中，构造了不同类型的标签节点将用户行为生成的同构图扩展为了异构图，再通过metapath2vec的方法基于定义的元路径进行随机游走。这提供了另外一种引入side information的方式。</p>
<h3 id="hin2vec">HIN2Vec</h3>
<p><a target="_blank" rel="noopener" href="https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiz-uXo0p7xAhVyqksFHSo2CYMQFjABegQIBRAD&amp;url=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3132847.3132953&amp;usg=AOvVaw0QleQ3h0R4RlhDkFTqvbfH">论文原文</a></p>
<p>除此之外，HIN2Vec中提出了另外一种可以保持异构网络语义信息的方法：以图中多种关系类型为label训练一个多分类模型，来预测任意两个节点之间可能存在的关系。与metapath2vec类似，HIN2Vec也默认元路径中包含了异构图中的多种关系信息，因此利用这部分信息指导embedding的生成。不同的是，HIN2Vec没有在随机游走的过程中以元路径作为指导，而是依然以节点进行随机游走生成节点序列，将metapath定义为图中的关系类型集合，作为label训练多分类器。HIN2Vec的最初设想是，给定节点对 <span class="math inline">\(&lt;x, y&gt;\)</span> ，预测节点之间的存在系集合<span class="math inline">\(R\)</span>中的哪几种关系。然而，找寻所有可能的关系是不现实的。因此。HIN2Vec将多分类问题简化为二分类问题：</p>
<img src="/2021/06/16/Graph-Embedding/hin2vec.png" class="">
<p>以上图为例，关系<span class="math inline">\(r\)</span>取自预先定义的由元路径组成的关系集合<span class="math inline">\(R\)</span>。HIN2Vec通过判断节点对<span class="math inline">\(&lt;x, y&gt;\)</span> 之间是否存在关系<span class="math inline">\(r\)</span>来训练一个二分类模型，将异构网络中的信息集成在分类器的权重<span class="math inline">\(W\)</span>中。</p>
<!-- question: metapath真的可以包含异构图的关系信息吗？ -->
<hr>
<p>值得一提的是，虽然上述方法的有效性已经得到了大量学术实验的验证，但在实际应用中，仍然有关键的一点是我们关注的，那就是“<em>如果有新加入的节点，删除的节点，新加入的边等问题要怎么处理呢？</em>”这一<strong>动态性图嵌入</strong>(<strong>Dynamic Network Embedding</strong>)的问题。这篇博客不会展开篇幅去讲这一领域的方法，在这里只提到一篇我看过的论文<a href="">Dynamic Network Embedding - An Extended Approach for Skip-gram based Network Emebdding</a>中提出的解决方案。文中对图嵌入方法常用的目标函数进行了数学上的分解(<em>decomposable objective</em>)，使原本的目标函数可以更新局部节点，而不是更新全部。接下来，作者又计算出了每次图结构发生变化时被影响最大的是哪些节点，以便有针对性地进行更新。修改过的目标函数可以很快计算出新加入的节点的representation，又可以对影响最大的节点进行局部更新。</p>
<h2 id="结语">结语</h2>
<p>metapath作为一种先验知识，可以很好地指导embedding的训练过程。然而，无论是随机游走，还是metapath指导下的随机游走，展开的序列信息已经一定程度上失去了图数据本身的结构信息。那么，有没有一种方式可以直接在图的拓扑结构上聚合特征，而不是先把二维结构展开成一维序列？这种方式和Graph Embedding相比又有什么优缺点呢？下一篇会介绍基于<strong>图神经网络</strong>(<strong>Graph Neural Network</strong>)的图嵌入方法。</p>
<hr />

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/14/Embeddings-in-Recommedation-System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/14/Embeddings-in-Recommedation-System/" itemprop="url">Embeddings in Recommendation System</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-14T10:40:26+03:00">
                2021-05-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/" itemprop="url" rel="index">
                    <span itemprop="name">paper_reviews</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.5.14</i> </font>
</p>
<hr>
<p>This is the draft version, with more detailedly explanation.</p>
<p>本文主要介绍了xxx</p>
<p>本文目录如下： ## Matrix Factorization</p>
<h3 id="svd">SVD</h3>
<p>基于矩阵分解的嵌入方法最初来自于伪奇异值分解。以user-item-rating这一矩阵为例，奇异值分解将大矩阵<span class="math inline">\(R:m \times n\)</span> 分解为两个小矩阵<span class="math inline">\(U:m \times k\)</span>，<span class="math inline">\(I:k \times n\)</span> 的乘积：</p>
<p><span class="math display">\[R = U\times I\]</span></p>
<p>这其中的k维向量，代表隐因子（Latent Factor）向量。矩阵分解认为，隐因子一定程度上代表了user和item的某种属性。</p>
<p>这样做的理论依据，源自线性代数中的特征值分解。对于<span class="math inline">\(n\)</span>阶方阵<span class="math inline">\(A\)</span>，存在常数 <span class="math inline">\(\lambda\)</span>，非零列向量 <span class="math inline">\(\alpha\)</span>：</p>
<p><span class="math display">\[A\alpha = \lambda\alpha\]</span></p>
<p>其中，<span class="math inline">\(\lambda\)</span>为方阵A的特征值，<span class="math inline">\(\alpha\)</span>为特征向量。矩阵与另一个矩阵的乘积实际上代表了一次<strong>线性变换</strong>，即将原本的矩阵映射到了另外的一个向量空间。但在上面的式子中，A乘以 <span class="math inline">\(\alpha\)</span>的结果和乘以常数 <span class="math inline">\(\lambda\)</span>是一样的。说明对于矩阵A而言存在着一些不变的值，即特征向量 <span class="math inline">\(\alpha\)</span>。</p>
<p>特征分解仅限于方阵，对于一般的矩阵上有相似的性质，即<strong>奇异值分解</strong> <strong>SVD（Singular Vector Decomposition）</strong>。任何矩阵，都可以通过SVD的方式分解成几个矩阵相乘的形式：</p>
<p><span class="math display">\[R = U\times S\times V\]</span></p>
<p>其中，<span class="math inline">\(U\)</span>是一个<span class="math inline">\(m\times m\)</span>正交矩阵，<span class="math inline">\(S\)</span>是特征值矩阵，<span class="math inline">\(V\)</span>是一个<span class="math inline">\(n\times n\)</span>正交矩阵。如果我们只取前k个大的特征值，并将其中两个矩阵合并只留两个矩阵，就会得到我们一开始的形式：</p>
<p><span class="math display">\[R = U\times I\]</span></p>
<p>在user-item-rating这一特定的矩阵中，这两个子矩阵可以分别代表user的偏好程度和item的成分属性。将两个矩阵相乘，即可得到评分矩阵。</p>
<p>在实际应用的过程中，对于user-item-rating矩阵而言，评分的预测实际上是一个补全矩阵的过程。原本的矩阵中，有一部分有评分，而有一部分没有。因此，矩阵分解的目标是使得最终得到的两个小矩阵的乘积可以很好地补全大矩阵中没有评分的未知项，如何获得这两个最优的小矩阵即如何学习到满意的可以代表user、item属性的嵌入向量矩阵。我们使有评分的部分与预测得到的评分误差最小即可，具体的目标函数取决于这一误差损失的定义，常用的为MSE。</p>
<p>这是最简单的SVD，它还有很多变种，包括biasSVD，SVD++等等。</p>
<!-- ### BPR -->
<h3 id="factorization-machine">Factorization Machine</h3>
<p><strong>分解机FM（Factorization Machine）</strong>的理论基础源自广义线性模型和矩阵分解。它的目标是解决数据稀疏情况下如何对高阶特征进行组合的问题（estimate interactions within features even with huge sparsity）。下文以二阶为例。</p>
<p>在普通的线性模型中，一般对每个特征<span class="math inline">\(x_i\)</span>进行一阶的线性组合来得到预测结果 <span class="math inline">\(\hat{y}\)</span>：</p>
<p><span class="math display">\[\hat{y(x)}=w_0+\sum_{i=1}^nw_ix_i\]</span></p>
<p>然而，这样简单的一阶组合会损失掉不同特征之间的关联信息。因此，可以引入二阶关联特征：</p>
<p><span class="math display">\[\hat{y(x)}=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nw_{i,j}x_ix_j\]</span></p>
<p>引入一个对称矩阵<span class="math inline">\(W\)</span>，其中<span class="math inline">\(w_{i, j}\)</span> 代表了特征<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>之间的关联权重。然而，这样的组合面临一个很大的问题。因为只有当<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>都不为0时，<span class="math inline">\(w_{i, j}\)</span> 的取值才有意义，否则无法对这个参数进行估计。这在样本过于稀疏时是无法保证的，会导致特征关联权重矩阵<span class="math inline">\(W\)</span>得不到充分的训练。</p>
<p>FM正是为了解决这一问题。FM使用了矩阵分解的思想，将<span class="math inline">\(W\)</span>分解为<span class="math inline">\(W=V^TV\)</span>。对于<span class="math inline">\(W\)</span>中的每个参数，有：</p>
<p><span class="math display">\[w_{i,j}=&lt;v_i, v_j&gt;\]</span></p>
<p>其中，<span class="math inline">\(v_i\)</span>是第<span class="math inline">\(i\)</span>个特征的<span class="math inline">\(k\)</span>维隐向量，包含<span class="math inline">\(k\)</span>个隐因子。<span class="math inline">\(&lt;·,·&gt;\)</span>表示两个<span class="math inline">\(k\)</span>维向量的点积：</p>
<p><span class="math display">\[&lt;v_i, v_j&gt;=\sum_{f=1}^kv_{i,f}v_{j,f}\]</span></p>
<p>最终的公式为：</p>
<p><span class="math display">\[\hat{y(x)}=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n\sum_{f=1}^kv_{i,f}v_{j,f}x_ix_j\]</span></p>
<p>因此，对于这个模型而言，不再直接求解特征组合权重参数<span class="math inline">\(w_{i, j}\)</span> ，而是采用隐向量的点积，对每一个特征分量<span class="math inline">\(x_i\)</span>引入隐向量<span class="math inline">\(v_i=(v_{i,1}, ..., v_{i,k})\)</span>，利用<span class="math inline">\(v_iv_j^T\)</span>对<span class="math inline">\(w_i,j\)</span>进行估计：</p>
<p><span class="math display">\[\hat w_{i,j}=v_iv_j^T\]</span></p>
<p>对于<span class="math inline">\(x_kx_i\)</span>和<span class="math inline">\(x_ix_j\)</span>的系数&lt;<span class="math inline">\(v_k\)</span>, <span class="math inline">\(v_i\)</span>&gt;和&lt;<span class="math inline">\(v_i\)</span>, <span class="math inline">\(v_j\)</span>&gt;来说，它们之间都有<span class="math inline">\(v_i\)</span>，只要包含<span class="math inline">\(v_i\)</span>就可以对<span class="math inline">\(v_i\)</span>进行训练，即使<span class="math inline">\(x_kx_i\)</span>之间不存在带有关联信息的样本，所有包含<span class="math inline">\(x_i\)</span>的非零组合特征（存在某个<span class="math inline">\(j \neq i\)</span>使得<span class="math inline">\(x_ix_j \neq 0\)</span>）的样本都可以用来学习隐向量<span class="math inline">\(v_i\)</span>，这在很大程度上缓解了数据稀疏所带来的参数学习不充分的问题。</p>
<!-- 以实际情况举个例子。 -->
<h2 id="embeddings">embeddings</h2>
<p>另外一类对图中信息进行表示学习的受NLP中的词向量模型启发。我们知道，NLP中词向量模型的目标是希望能够找到这样的一个低维度的向量空间，可以很好地表示语库中每个词语的特征，使得通过这些低维度的向量即可以计算出词之间的语义相似性。这个特征空间就是词向量组成的特征空间。</p>
<p>对于自然语言来说，出现在同一个句子中的词，彼此之间由于上下文的语义关系，具有一定的关联特征。<strong>a graph!!!</strong> 这种句子之间的上下文信息对于好的词向量生成至关重要。word2vec正是基于这一点。在把这一思路运用到graph embedding之前，我们先介绍一下word2vec的原理。</p>
<h3 id="理论基础word2vec">理论基础：word2vec</h3>
<p>原始的word2vec模型中提供了两种方法，分别是以中心词向量生成背景词向量概率分布的skip-gram，和以背景词向量生成中心词向量概率分布的c-bow。 #### skip-gram 以skip-gram为例，词库大小为<span class="math inline">\(T\)</span>，<span class="math inline">\(w_c\)</span>为中心词，<span class="math inline">\(w_o\)</span>为背景词。以<span class="math inline">\(P(w_o|w_c)\)</span> 代表背景词<span class="math inline">\(w_o\)</span>的概率分布，<span class="math inline">\(u\)</span>代表背景词向量，<span class="math inline">\(v\)</span>代表中心词向量，定义如下：</p>
<p><span class="math display">\[P(w_o|w_c) = \frac{exp(u_o^Tv_c)}{\sum_{i\in T}exp(u_i^Tv_c)}\]</span></p>
<p>根据最大似然估计的思想，我们的目标是最大化当前中心词的所有背景词向量的联合概率分布，即：</p>
<p><span class="math display">\[argmax  \frac{1}{T}\prod_{t=1}^T \prod_{-m\leq j\leq m}log P(w^{t+j}_o|w^t_c)\]</span></p>
<p>其中，m为背景词窗口大小。</p>
<p>取负并取对数，得到损失函数：</p>
<p><span class="math display">\[L=  -\frac{1}{T}\sum_{t=1}^T \sum_{-m\leq j\leq m}log P(w^{t+j}_o|w^t_c)\]</span></p>
<p>采用随机梯度下降的方式进行参数优化： <span class="math display">\[
\begin{matrix}
v_c = v_c - \frac{\partial L} {\partial v_c} \\
\frac{\partial L} {\partial v_c} = u_o-\sum_{j \in T}P(w_o^j|w_c)u_j
\end{matrix}
\]</span></p>
<p>在求解背景词的概率分布时使用了softmax，可以看到，由于背景词可能是词典中的任意一个词，因此导致我们要对整个词典大小的词进行遍历，这样的计算代价显然是不现实的。word2vec中提出了两种优化训练的方法，分别是<strong>负采样</strong>（<strong>negative sampling</strong>）和层序softmax。</p>
<h4 id="负采样">负采样</h4>
<p>负采样首先基于两个假设：</p>
<blockquote>
<ul>
<li>中心词<span class="math inline">\(w_c\)</span>和背景词<span class="math inline">\(w_o\)</span>同时出现在一个训练窗口。</li>
<li>中心词<span class="math inline">\(w_c\)</span>和第k个噪声词<span class="math inline">\(w_k\)</span>不同时出现在一个训练窗口。</li>
</ul>
</blockquote>
<p>以skip-gram为例，中心词和背景词同时出现在当前训练窗口的概率为：</p>
<p><span class="math display">\[
P(D=1|w_c, w_o) = \sigma (u_o^T \cdot v_c)
\]</span></p>
<p>同理，中心词和噪声词没有同时出现在当前训练窗口的概率为：</p>
<p><span class="math display">\[
P(D=0|w_c, w_k) = 1 - \sigma (u_k^T \cdot v_c)
\]</span></p>
<p>因此，根据MLE的思想，最大化联合概率分布：</p>
<p><span class="math display">\[
P(w_o|w_c) = P(D=1|w_c, w_o) \prod_{k=1}^K P(D=0|w_c, w_k)
\]</span></p>
<p>取对数再取负，把上述概率计算公式代入，激活函数为sigmoid，优化后的损失函数变为： <span class="math display">\[
L = -log \frac {1} {1+e^{-u_o^T \cdot v_c}} - \sum_{k=1, w_k~P(w)}^K log \frac {1} {1+e^{u_k^T \cdot v_c}}
\]</span></p>
<p>由此可知，如果我们想将这一套训练方法应用到其他的场景中，那么需要明确的有两点：</p>
<blockquote>
<ul>
<li>如何构造有意义的sequence。在词向量的训练模型中，sequence指的是语料库中的所有句子。这些句子提供了词语之间的上下文关系，是学习语义相似性的基础。</li>
<li>如何结合具体应用场景对训练过程采用负采样或层序sofrmax优化。</li>
</ul>
</blockquote>
<p>接下来将词向量的训练原理类比到graph embedding中。给定图<span class="math inline">\(G=(V, E)\)</span>，其中<span class="math inline">\(V\)</span>代表节点集合，<span class="math inline">\(E\)</span>代表图中边集合。定义变量<span class="math inline">\(Type_v\)</span>，<span class="math inline">\(Type_e\)</span>，分别代表图中节点类型集合和边类型集合。</p>
<p>对于图中节点<span class="math inline">\(u \in V\)</span> ，我们希望找到一种方法<span class="math inline">\(f:u\rightarrow R^d\)</span>，以<span class="math inline">\(R^d\)</span>中的向量<span class="math inline">\(e_u\)</span>来代表节点<span class="math inline">\(u\)</span>的特征。我们希望可以充分利用图中的连接关系（类比于句子中词语之间的上下文关系），来学习到每个节点特征的低维度表示。</p>
<p>在这一理论基础下，我们接下来看几大经典的基于word2vec的graph embedding算法。首先是对同构图(Homogenous Network)信息进行嵌入的<strong>deepWalk</strong>和<strong>node2vec</strong>。两者都采用<strong>随机游走</strong>（<strong>Random Walk</strong>）的方法生成节点序列，即可以认为：</p>
<blockquote>
<ul>
<li>两个同时出现在一个随机游走序列中的节点之间存在一定的相似性。这种相似性就对应词嵌入模型中的上下文信息。</li>
</ul>
</blockquote>
<!-- ## 同构图嵌入 -->
<h3 id="deepwalk">deepWalk</h3>
<p>给定无向无权图<span class="math inline">\(G=(V, E)\)</span>，<span class="math inline">\(V\)</span>是xxx，例如在xx中，<span class="math inline">\(V\)</span>可能是xx，deepWalk首先给定游走策略<span class="math inline">\(R\)</span>，以节点<span class="math inline">\(u\)</span>为起始点进行随机游走，生成的所有序列中包含的所有节点组成了节点<span class="math inline">\(u\)</span>的<strong>邻域</strong>(<strong>neighborhood</strong>)<span class="math inline">\(N_{R(u)}\)</span>。对应word2vec的skip-gram算法，<span class="math inline">\(N_{R(u)}\)</span> 中的节点<span class="math inline">\(v\)</span>即为背景词，<span class="math inline">\(u\)</span>即为中心词。因此，deepWalk的损失函数可以定义为：</p>
<p><span class="math display">\[
\begin{matrix}
L = \sum_{u \in V}\sum_{v \in N_{R(u)}} -log(P(e_v|e_u))\\
P(e_v|e_u) = \frac {exp(e_u^T \cdot e_v)} {\sum_{j\in V}(e_u^T \cdot e_j)}
\end{matrix}
\]</span></p>
<p>其中，<span class="math inline">\(e_u\)</span>代表节点<span class="math inline">\(u\)</span>的embedding。由于<span class="math inline">\(P(e_v|e_u)\)</span> 的计算是不现实的，因此deepWalk采用Hierarchical Softmax对训练过程进行优化。deepWalk用所有节点构建了一棵树，每个节点是这棵树的叶子节点，因此到每个节点都有一条唯一的路径。层序softmax把计算<span class="math inline">\(P(e_v|e_u)\)</span> 的问题转化成最大化层序遍历的过程中最大化某一条路径的方式(maximize the probability of a specific path in the hierarchy)。如果到节点<span class="math inline">\(u_k\)</span>的路径为( <span class="math inline">\(b_0\)</span>, <span class="math inline">\(b_1\)</span>, ..., <span class="math inline">\(b_{log|V|}\)</span>) (<span class="math inline">\(b_0 = root\)</span>, <span class="math inline">\(b_{log|V|} = u_k\)</span>)，那么</p>
<p><span class="math display">\[
\begin{matrix}
P(e_v|e_u) = \prod_{l=1}^{log|V|}P(b_l|e_u)\\
P(b_l|e_u) = \frac {1} {1+e^{-e_u \cdot {e_{b_l}}}}
\end{matrix}
\]</span></p>
<!-- $$
\begin{matrix}
P(u_k|\phi(v_j)) = \prod_{l=1}^{log|V|}P(b_l|\phi (v_j))\\
P(b_l|\phi (v_j)) = \frac {1} {1+e^{-\phi (v_j) \cdot {}}}
\end{matrix}
$$ -->
<p>其中，<span class="math inline">\(e_{b_l}\)</span> 代表了树中节点<span class="math inline">\(b_l\)</span>的父节点向量。由于树的结构性质，层序softmax把原本<span class="math inline">\(O(|V|)\)</span> 的计算代价建为了<span class="math inline">\(O(log|V|)\)</span>。</p>
<!-- 同样的，计算代价是过高的$O||V^2||$， -->
<!-- 因此deepWalk采用negative sampling进行优化。

$$L = log(\sigma (e_u^T \cdot e_v))-\sum_{i=1}^klog(\sigma (e_u^T \cdot e_i))$$ -->
<p>P.S. 理论上deepWalk适用于边无权重的图，但在实际应用的过程中，可以把边的权重和随机游走的转移概率结合在一起，所以也比较灵活。这也是在deepWalk的基础上产出的诸多衍生模型的做法。</p>
<h3 id="node2vec">node2vec</h3>
<p>node2vec对deepWalk的随机游走序列生成过程进行了改进。deepWalk中选择游走的下一个节点的方式是均匀的，而这样生成的序列可能无法捕捉到相隔较远的两个节点之间的相似性，因为它们很难出现在同一个随机游走序列中。</p>
<p>node2vec采用了更加灵活的随机游走策略来生成邻域<span class="math inline">\(N_{R(u)}\)</span>，使得捕捉到的相似性信息可以在<strong>局部信息</strong>和<strong>全局信息</strong>之间进行一个权衡。我们知道，对图的遍历有两种策略，分别是<strong>BFS</strong>（深度优先搜索）和<strong>DFS</strong>（广度优先搜索）。这两种策略恰恰对应了两种不同的定义节点邻域的方式：</p>
<ul>
<li><p>BFS: 局部信息(local microscopic view)</p></li>
<li><p>DFS: 全局信息(global macroscopic view)</p></li>
</ul>
<p>因此，node2vec给定两个超参数用以指导随机游走：</p>
<ul>
<li><p>return parameter \<span class="math inline">\(p\)</span>: 控制返回到上一个节点</p></li>
<li><p>in-out parameter \<span class="math inline">\(q\)</span>: 控制是否远离当前节点（DFS）或靠近当前节点（BFS）</p></li>
</ul>
<p>这两个超参数控制随机游走的转移概率。<span class="math inline">\(1/p\)</span>控制着返回上一个节点的概率。<span class="math inline">\(p\)</span>越小，随机游走生成的序列越接近于BFS遍历，序列所捕获的邻域信息越接倾向于局部信息；<span class="math inline">\(1/q\)</span>控制着更远一步的概率。<span class="math inline">\(q\)</span>越小，随机游走生成的序列越接近于DFS遍历，序列所捕获的邻域信息越接倾向于全局信息。前者更适用于捕捉community之间的相似性，后者更适合于捕捉图中的层次化/结构化信息:</p>
<!-- 
<p>--&gt;</p>
<!-- \begin{figure}

\end{figure} -->
<p>在实际应用中，我们可以根据场景和图挖掘任务的不同对这两个参数进行设置。例如，</p>
<hr>
<p>deepWalk和node2vec是NLP领域的文本嵌入模型在图结构上的一种实现，它们给很多已经在工业界落地的应用模型提供了理论基础。deepWalk出现后，有许多工业界模型都在它的基础上结合业务场景进行了改进，在实际应用上取得了不错的效果。</p>
<h3 id="衍生模型">衍生模型</h3>
<h4 id="airbnb---real-time-personalization-using-embeddings-for-search-ranking-at-airbnb">Airbnb - Real-time Personalization using Embeddings for Search Ranking at Airbnb</h4>
<p>Airbnb是一个民宿平台，同时对房东和租客开放。房东可以在平台上发布房源信息，租客通过搜索选取自己心仪的房源(<strong><em>list</em></strong>)进行预定。同时，房东也可以决定是否接受租客的预定。Airbnb的目标在于，如何在搜索中<strong>实时</strong>向用户推荐更符合他们<strong>偏好</strong>（preference）的房源信息。论文把这个问题具体化为，如何基于几种平台上有效的用户和租客行为<strong>点击</strong>(click)，<strong>预定</strong>)(booking)，<strong>拒绝预定</strong>(reject)生成有效的list embeddings。</p>
<p>Airbnb把用户偏好分为了<strong>短期兴趣</strong>(short-term interest)和<strong>长期兴趣</strong>(long-term interest)。短期兴趣捕捉用户实时搜索浏览的房源特征，长期兴趣则更多从用户的预定历史中挖掘更普遍的兴趣偏好，例如一个当前在浏览巴黎房源信息的租客，他过去在洛杉矶预定的房源特征同样可以提供一种偏好信息(cross-market similarities)。</p>
<!-- 其中，短期兴趣为实时采集一个固定时间窗口内用户点击list的序列(click session)，论文中设定的时间窗口为30min。
Airbnb通过click session生成了list sequences，包含了用户短期内对于list的偏好信息。在训练过程中，Airbnb结合自身业务场景进行了：
 -->
<h5 id="短期兴趣">短期兴趣</h5>
<p>在word2vec的部分我们说过，把词向量嵌入应用到别的场景下的一个关键就是如何生成包含有效信息的序列，通过学习这部分有效的“上下文”信息来获取embeddings。Airbnb结合自身业务场景的embedding策略可总结为以下三点：</p>
<ul>
<li>每个租客30min内点击的list为一个click session，即由list组成的序列。序列对应词嵌入模型中的“句子”，每个list对应句子中的“词语”，点击信息对应“上下文信息”；</li>
<li>把每个租客的历史预定作为booked list，假设booked list始终与central list有关；</li>
<li>租客一段时间内的点击行为会有一个特殊的location偏好。例如一个准备去巴黎的租客不会浏览伦敦的房源信息。为了缓解负采样中大量负样本极有可能来源于不同地区这一问题，在目标函数引入当前central list相同位置的负样本集。</li>
</ul>
<p>接下来，套入到word2vec中，以skip-gram + negative sampling为例我们直接给出目标函数。其中，l为当前的central list，对应中心词<span class="math inline">\(w_c\)</span>；c为context list，对应背景词<span class="math inline">\(w_o\)</span>。<span class="math inline">\(D_p\)</span>为正样本集，<span class="math inline">\(D_n\)</span>为负样本集。<span class="math inline">\(D_{m_n}\)</span> 为采样自与当前click list相同location的负样本集。</p>
<img src="/2021/05/14/Embeddings-in-Recommedation-System/airbnb_objfunc.png" class="">
<!-- $$
argmax_{\theta} \sum_{(l, c)\in D_p} log \frac {1}{1+e^{-v^{'}_cv_l}} +
\sum_{(l, c)\in D_n} log \frac {1}{1+e^{v^{'}_cv_l}} +
log \frac {1}{1+e^{-v^{'}_{l_b}v_l}} +
\sum_{(l, m_n)\in D_{m_n}} log \frac {1}{1+e^{v^{'}_{m_n}v_l}}
$$ -->
<p>除此之外，Airbnb还指出了如何在短期兴趣的学习中解决冷启动的问题。如果有新的list出现，则在附近(within a 10 miles radius)寻找三个相同类型，价格接近的lists，将它们embedding的平均作为new list的embedding。</p>
<h5 id="长期兴趣">长期兴趣</h5>
<p>到这里，Airbnb通过click session成功学习到了short-term interest的list embedding。然而，对于长期兴趣(booking history)却很难用book session去训练，原因是我们没有足够的数据，这在训练过程中会带来严重的数据稀疏问题。因此，Airbnb不再直接学习user embeddings和list embeddings，而是转而学习<strong><em>user_type embeddings</em></strong>和<strong><em>list_type embeddings</em></strong>。这相当于提前进行了一个类型聚类，可以一定程度上解决冷启动问题。</p>
<p>以论文中的表格为例，无论是用户还是房源都会存在基础属性信息，Airbnb把这些基础信息组合起来表示user_type和list_type。例如一个美国的房源，房源基础信息为bucket 1，每晚价格60美金为bucket 3，则该房源的<span class="math inline">\(listing\_type = US\_lt_1\_pn3\)</span>。user_type同理。因此，Airbnb把book session中的sequence定义为user_type和list_type的组合--&gt;(<span class="math inline">\(u_{type_1}l_{type_1}, ..., u_{type_M}l_{type_M}\)</span>)，同一user的user_type可能会随时间变化而有所不同。</p>
<img src="/2021/05/14/Embeddings-in-Recommedation-System/list_type_table.jpeg" class="">
<p>有了sequence之后就继续延用word2vec + negative sampling的训练模式，如下图所示，图中红色部分为房主的reject信息，与上述负采样信息类似，旨在用这部分信息降低租客被拒绝的概率。由于sequence中既有user_type又有list_type，因此训练过程实际上是把它们当成了完全相同的节点，映射到了同一个向量空间里。这里的处理方式我<strong>暂时存疑</strong>。</p>
<img src="/2021/05/14/Embeddings-in-Recommedation-System/long_term_train.png" class="">
<hr>
<!-- Airbnb将短期兴趣学习到的list_embedding用于实时的搜索推荐，将长期兴趣学习到的user_type_embedding和list_type_embedding的cosine相似度 -->
<p>Airbnb将点击行为序列直接用于词嵌入模型的输入。然而，这样的训练数据可能是不充分的。例如，本来相似的item在独立的用户行为序列中可能并不存在共现，用户1可能浏览了ABC，用户2可能浏览了CD，在训练数据中并不存在的ABCD序列，这可能会使模型不能学习到A与D以及B与D的这部分相似性信息。</p>
<!-- 此外，对于冷启动的问题，Airbnb -->
<h4 id="eges---billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba">EGES - Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</h4>
<p>阿里的这篇论文主要在于解决推荐中的召回部分。其中亮点在于引入side information对冷启动问题的解决。</p>
<p>在学习item_embeddings时，阿里同样使用了基于用户session生成的序列作为词嵌入模型的输入。与Airbnb不同的是，阿里没有直接使用用户的点击序列，而是用不同用户的点击行为先组成一个有向图。如下图所示，用户U1在一个时间窗口内先后点击了D和A，则建立一条由D指向A的边。如果有另一个用户也有相同顺序的点击行为，那么D和A之间边的权重会加1。在基于session构造了item graph后，利用deepWalk的思想，先进行随机游走生成序列，再利用词嵌入模型进行训练。</p>
<img src="/2021/05/14/Embeddings-in-Recommedation-System/eges_frm.png" class="">
<p>然后上述基于用户行为的建图及游走过程无法解决冷启动问题，对于用户行为很少的item并不能输出满意的embedding结果。因此，这篇论文在训练阶段引入了item的side information作为可学习的特征，在用户行为很少的情况下期望利用这部分特征来对item进行有效的表示。如下图所示，<em>SI 0</em>为item本身的特征，即上述session中学到的embedding。对于新item这部分可以初始化为0向量。<em>SI 1,... SI n</em>为不同类型的side information，</p>
<img src="/2021/05/14/Embeddings-in-Recommedation-System/side_info.png" class="">
<p>在隐含层对代表不同信息的embedding进行了聚合。论文中给出了两种聚合方法，一种是普通的average pooling，另一种则是加权平均。后者在特征融合阶段又增加一个可训练的权重矩阵，考虑不同信息对于最后得到的item_embedding的重要程度。这里其实已经有了应用attention的初步想法，论文的future work里也提到了这一点。</p>
<p><strong><em>在NLP中，一个句子中词的存在天然的语义关系，因此word2vec所学习到的这部分序列信息是有意义的。然而在推荐系统中，用户的点击行为之间可能并不存在很强的序列关系。？？？？</em></strong></p>
<hr>
<p>以上两种图嵌入的方法都是对同构图(Homogenous Network)的处理，即节点类型只有一种的图：对于图<span class="math inline">\(G\)</span>，有 <span class="math inline">\(|Type_v| = 1\)</span> &amp; <span class="math inline">\(|Type_e| = 1\)</span>。然而我们常见的很多图结构都是异构图(Heterogenous Network)，即节点类型不止有一种的图：对于图<span class="math inline">\(G\)</span>，<span class="math inline">\(|Type_v| \geq 1\)</span> or <span class="math inline">\(|Type_e| \geq 1\)</span>。</p>
<p>deepWalk等基于节点进行随机游走的方法，在涉及信息更复杂的异构图时，表现可能很差。在异构图中，节点和边共同构成完整的语义信息(semantics)，基于节点的随机游走会割裂这种关系。以下图学术网络这个典型的异构图为例，网络中存在四种节点类型，不同节点之间的边包含不同的语义信息。例如，author节点<span class="math inline">\(a_1\)</span>和paper节点<span class="math inline">\(p_1\)</span>的边代表<span class="math inline">\(a_1\)</span>是<span class="math inline">\(p_1\)</span>的作者，而<span class="math inline">\(a_1\)</span>和<span class="math inline">\(a_2\)</span>之间的边则代表了两人是coauthor的关系。由于author类型的节点数比较多，基于节点的随机游走生成的大部分序列中都会包含author节点。在限定游走的步数的前提下，数目比较少的节点类型如org出现的概率会很小。在异构图中特定语义信息约束下，会导致这部分节点及边信息训练数据不够，因此无法生成令人满意的embedding。</p>
<p>在异构图中，存在一种元路径的定义。同样以学术网络为例，APA和APVPA是两种metapath，前者代表了两个作者为一篇论文的coauthor，后者代表两个不同作者在同一个venue发表过论文。可以看出，元路径实际代表了不同类型节点或者相同类型节点之间存在的某种特定关系。</p>
<h3 id="metapath2vec">metapath2vec</h3>
<p><a href="#metapath2vec">metapath2vec</a>基于元路径对deepWalk的模式进行了改进。metapath2vec的基本架构和deepWalk类似，即随机游走生成序列加上skip gram训练模型，区别在于：</p>
<ul>
<li><p>metapath2vec采用了基于元路径的随机游走来生成序列。提前定义的元路径作为一种先验信息在指导着随机游走，使得生成的序列都是符合图中原本语义的，这很好地捕捉了图中的异构信息。</p></li>
<li><p>metapath2vec采用了异构skip gram模型。如下图公式阴影部分所示，与普通的skip gram的区别在于定义了不同类型节点的异构邻域。</p></li>
</ul>
<img src="/2021/05/14/Embeddings-in-Recommedation-System/hete_skip_gram.png" class="">
<!-- 我在查阅资料的过程中，在KM上看到了微视团队发的一篇graph embedding结合具体业务场景模型落地的文章([weishi]: url "title")。微视团队在实验过程中，构造了不同类型的标签节点将用户行为生成的同构图扩展为了异构图，再通过metapath2vec的方法基于定义的元路径进行随机游走。这提供了另外一种引入side information的方式。 -->
<h3 id="hin2vec">HIN2Vec</h3>
<p>除此之外，论文[HIN2Vev]中提出了另外一种可以保持异构网络语义信息的方法：以图中多种关系类型为label训练一个多分类模型，来预测任意两个节点之间可能存在的关系。与metapath2vec类似，HIN2Vec也默认元路径中包含了异构图中的多种关系信息，因此利用这部分信息指导embedding的生成。不同的是，HIN2Vec没有在随机游走的过程中以元路径作为指导，而是依然以节点进行随机游走生成节点序列，将metapath定义为图中的关系类型集合，作为label训练多分类器。HIN2Vec的最初设想是，给定节点对 <span class="math inline">\(&lt;x, y&gt;\)</span> ，预测节点之间的存在系集合<span class="math inline">\(R\)</span>中的哪几种关系。然而，找寻所有可能的关系是不现实的。因此。HIN2Vec将多分类问题简化为二分类问题：</p>
<img src="/2021/05/14/Embeddings-in-Recommedation-System/hin2vec.png" class="">
<p>以上图为例，关系<span class="math inline">\(r\)</span>取自预先定义的由元路径组成的关系集合<span class="math inline">\(R\)</span>。HIN2Vec通过判断节点对<span class="math inline">\(&lt;x, y&gt;\)</span> 之间是否存在关系<span class="math inline">\(r\)</span>来训练一个二分类模型，将异构网络中的信息集成在分类器的权重<span class="math inline">\(W\)</span>中。</p>
<p>question: metapath真的可以包含异构图的关系信息吗？</p>
<hr>
<p>metapath作为一种先验知识，可以很好地指导embedding的训练过程。然而，图结构展开的序列信息终归只是低阶的局部信息，</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/02/recommender-system/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/02/recommender-system/" itemprop="url">recommender system</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-05-02T17:22:01+03:00">
                2021-05-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/" itemprop="url" rel="index">
                    <span itemprop="name">paper_reviews</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/recommender-system/" itemprop="url" rel="index">
                    <span itemprop="name">recommender_system</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.5.2</i> </font>
</p>
<hr>
<h2 id="stage-1">Stage 1</h2>
<h3 id="factorization-machines">Factorization Machines</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/HowardEmily/article/details/105786891">ref1./</a> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50426292">ref2./</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/04/14/GNNs-Related/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/04/14/GNNs-Related/" itemprop="url">GNNs_Related</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-04-14T08:16:15+03:00">
                2021-04-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/" itemprop="url" rel="index">
                    <span itemprop="name">paper_reviews</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.3.30</i> </font>
</p>
<hr>
<h2 id="ggnn"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.05493">GGNN</a></h2>
<h3 id="blogs-references">Blogs References:</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/108294485">ref1./</a> A very clear blog introduces the development from GNN to GGNN. The explanation about Fixed Point Theorem and Contraction Map is quite easy to understand.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/04/04/Neural-Graph-CF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/04/04/Neural-Graph-CF/" itemprop="url">Neural-Graph-CF</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-04-04T14:34:59+03:00">
                2021-04-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/" itemprop="url" rel="index">
                    <span itemprop="name">paper_reviews</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.4.4</i> </font>
</p>
<hr>
<h3 id="motivation">Motivation</h3>
<p>This paper proposes an efficient way to learn much more representative embeddings in collaborative filtering. The traditional matrix factorization way is not sufficient to aggregate collaborative signal, which is the high-order connectivity, in the user-item interaction network. As a result, the embeddings could not capture enough information for ranking.</p>
<h3 id="introduction">Introduction</h3>
<p>This paper utilizes graph neural network with multiple embeddings layers to capture high-order connectivity relations, with also a prediction layer to aggregate the refined embeddings and output the affinity score of a user-item pair. The proposed model, named NGCF, uses the layer-wise propagation rule through multiple embeddings layers and uses the gradient of pair-wise BPR loss function to update the weights matrix.</p>
<h3 id="contributions">Contributions</h3>
<h4 id="first-contribution">First contribution</h4>
<p>This paper demonstrates the importance of high-order connectivity in embedding function of CF model.</p>
<h4 id="second-contribution">Second contribution</h4>
<p>It proposes NGCF model, a recommendation system based on graph neural network, to efficiently encode the collaborative signal in the form of high-order connectivity.</p>
<h4 id="third-contribution">Third contribution</h4>
<p>It evaluates and compares the performance of NGCF and different kinds of state-of-art models for recommendation system on three big real-world datasets, and also shows us the process for parameters tuning.</p>
<h3 id="solution">Solution</h3>
<h4 id="section">1</h4>
<h4 id="section-1">2</h4>
<p>It uses the layer-wise propagation rule through multiple embeddings layers and uses the gradient of pair-wise BPR loss function to update the weights matrix.</p>
<h4 id="section-2">3</h4>
<h3 id="critique">Critique</h3>
<h4 id="strength-1">Strength 1</h4>
<p>The GNN based model can well capture the high-order connectivity through multiple embedding layers propagation.</p>
<h4 id="strength-2">Strength 2</h4>
<p>In every embedding layer, NGCF obtains the current output embedding through message construction and message aggregation. First, like GCN, the message construction calculation in each layer is based on the normalized Laplacian matrix, which can well distribute the contribution of each nodes in the neighbourhood according to their respective degree. Second, unlike GCN, the embedding layer also encodes the interaction between e_i and e_u through pair-wise product of these two vectors. Third, unlike GCN, the activation function in message aggregation is LeakyReLU instead of ReLu, due to LeakyReLU's ability to encode both positive and small negative signals. Finally, unlike GCN, the normalized Laplacian matrix does not have self-connections. This is because in the message aggregation process, it has already aggregate the message from user to user. #### Strength 3</p>
<h4 id="weakness-1">Weakness 1</h4>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/30/Semi-GCN-Reading-Review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/30/Semi-GCN-Reading-Review/" itemprop="url">Semi-GCN_Reading_Review</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-30T12:14:37+03:00">
                2021-03-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reviews/" itemprop="url" rel="index">
                    <span itemprop="name">paper_reviews</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.3.30</i> </font>
</p>
<hr>
<h3 id="blogs-references">Blogs References:</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58178060">ref1./</a>, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/65276194">ref2./</a>, <a target="_blank" rel="noopener" href="https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780">ref3./</a></p>
<h3 id="motivation">Motivation</h3>
<!-- Describe the motivation of each of the papers. WHY is the addressed problem interesting and important to be solved?
WHAT is the connection between the papers you are discussing? -->
<h4 id="symbol-definitions">Symbol Definitions:</h4>
<p>Graph: G = (V, E), V and E denote the nodes set and edges set, respectively. Adjacency Matrix = A, with the size of |V|*|V| Degree Matrix = D, same size with A Normalized Laplacian Matrix = L = I-D^-1/2 A D^-1/2</p>
<h4 id="introduction">Introduction</h4>
<p>The traditional way to classify the nodes in a graph with only a small subset of nodes labels is to smoothing the label information through some form of graph based regularization. For example, a graph Laplacian regularization term in loss function can be defined as:</p>
<p>However, this method is limited due to the its assumption that the nodes connection, i.e., the edges, are more likely to represent the similarity of nodes. This is possibly wrong because the edges can be of many other meanings than the similarity. This paper, instead, tends to find a way to learn representations in graph structured data with the localized first-order approximation of spectral graph convolutions. GCN in this paper uses the layer-wise propagation process and can be stacked with several convolution layers.</p>
<h3 id="contributions">Contributions</h3>
<!-- Explain the main contributions of each of the papers. WHAT are the solved problems? The authors of this paper solve this and that problem. Their main contributions include this and that. blabla ... You may optionally use a list of contributions. For example: -->
<h4 id="first-contribution">First contribution:</h4>
<p>This paper puts up with a simple layer-wise propagation rule for representation learning on graph neural networks.</p>
<h4 id="second-contribution">Second contribution:</h4>
<p>It makes the convolution calculation more efficient with a first-order approximation of spectral graph convolutions.</p>
<h4 id="third-contribution">Third contribution:</h4>
<p>The algorithm is implemented into the task of nodes semi-supervised nodes classification and shows a fast and scalable results.</p>
<h3 id="solution">Solution</h3>
<!-- Very briefly explain HOW the authors solve the mentioned problems. -->
<h4 id="section">1</h4>
<p>Considering a two-layer GCN, the forward model is defined as:</p>
<p>Z = f(X, A) = softmax() A =</p>
<p>The loss function is cross-entropy, and the model only uses the known labels to do the gradient descent.</p>
<h4 id="section-1">2</h4>
<p>When considering the spectral convolutions on graphs, defined as:</p>
<p>g * x = U g</p>
<p>this paper uses the Chebyshev polynomials to approximate the convolution calculation, and only considers the first-order, i.e., the information within 1-hop neighbourhood of a node in one round of convolution calculation.</p>
<h4 id="section-2">3</h4>
<p>The model only uses the known labels to do the weight update.</p>
<h3 id="critique">Critique</h3>
<h4 id="strength-1">Strength 1</h4>
<p>The original calculation of spectral convolutions will cost a great expense. This paper uses the method demonstrated by xxxx[], to get the localized first-order approximation of Fourier Domain filter calculation given singular value matrix of normalized Laplacian matrix. This optimizes the calculation expense from O(N^2) to O(|E|), linear in the number of edges.</p>
<h4 id="strength-2">Strength 2</h4>
<p>Through the above calculation, spectral convolution no longer depends on the entire graph, but only on nodes within K steps from the center node. (In this paper, K is set as 1.) The model integrates the information in a more broader neighbourhood by stacking multiple convolution layers, but with a lay-wise propagation rule.</p>
<h4 id="strength-3">Strength 3</h4>
<p>The lay-wise propagation rule for model training saves a lot of calculation expense, also reduces the risk of gradient vanishing in deep architecture.</p>
<h4 id="strength-4">Strength 4</h4>
<p>The implement of normalized Laplacian matrix can decide the contribution weights of message passing of different nodes, based both on the connections and the nodes' degree. It can be simply understood by that if a node had a larger degree then the contribution weight will be less.</p>
<h4 id="weakness-1">Weakness 1</h4>
<p>The model cannot be used in the directed graphs and graphs with edges features.</p>
<h4 id="weakness-2">Weakness 2</h4>
<p>When adding the self-connection in adjacency matrix A, the model simply assumes equal importance of self-connections and edges to neighboring nodes.</p>
<h3 id="brainstorming">Brainstorming</h3>
<p>What are promising further research questions in the direction of the papers? How could they be pursued? An idea of a better model for something? A better algorithm? A test of a model or algorithm on a dataset or simulated data?</p>
<h4 id="section-3">1</h4>
<p>How this model can be used in edges with direction or features?</p>
<h4 id="section-4">2</h4>
<p>How this model can aggregate the information of nodes that are far away from the central node?</p>
<h3 id="conclusion">Conclusion</h3>
<p>Description of some promising further research directions and questions, and how could they be pursued.</p>
<p>We are considering to</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/29/Preparations-for-Interview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/29/Preparations-for-Interview/" itemprop="url">Preparations_for_Interview</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-29T16:20:14+03:00">
                2021-03-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/coding/" itemprop="url" rel="index">
                    <span itemprop="name">coding</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——since 2021.3.29</i> </font>
</p>
<hr>
<h2 id="basic-machine-learning-knowledge">Basic Machine Learning Knowledge</h2>
<h3 id="decision-tree">Decision Tree</h3>
<p>ID3: 以信息增益为切分依据。缺点是如果有一个特征它的取值很多那它的信息增益通常就很大。</p>
<p>C4.5：采用信息增益比，为了避免上面提到的ID3存在的问题</p>
<p>CART：基尼系数。一定是二叉树。对于每一个特征A，A的每一个取值a，计算数据集中A等于和不等于a的两个数据集的基尼系数，选择最小的特征</p>
<h4 id="随机森林">随机森林</h4>
<p>Random Forest以Cart树为基分类器。RF是典型的ensemble learning（集成学习），用的是bagging。bagging是指每次取出数据集的一部分作为子集，生成一棵完全生长的决策树，最后的结果由：1）如果是分类问题采用voting（投票）；2）回归问题则采用average（平均）产生。</p>
<p>如何防止决策树的过拟合：pruning （剪枝）</p>
<p>如何观察决策树是否过拟合：可以采用 1）划分training set和validation set，观察validation error，事实上这是通用的监测模型过拟合的方式；2）可以采用out-of-bag error，这部分其实和validation error是同样的道理。因为随机森林采用bagging，每次都是有放回抽样，所以在整个数据集中有一部分数据集是有可能从来都没有被选中用于生成树的，这部分unseen data可以作为validation set来观察模型是否过拟合。</p>
<h4 id="gbdt与xgboosting-梯度下降树">GBDT与XGboosting （梯度下降树）</h4>
<p>两种都是采用boosting，串行集成学习方式。不同的地方在于，后者的目标函数中采用牛顿法而不是梯度下降法，进行了泰勒二级展开。牛顿法用二次曲面去拟合局部曲面，而梯度下降法采用平面去拟合。前者拟合程度更好一些。</p>
<h3 id="逻辑回归">逻辑回归</h3>
<p>逻辑回归实际上是一种binary classification model（二分类模型）。</p>
<p>逻辑回归的损失函数：交叉熵。 为什么用交叉熵？这里有个推导，求解损失函数的梯度下降时，交叉熵会得到残差项（y-y‘），使得每次的权重更新项有一个性质：误差大时权重更新快，误差小时权重更新快。这是一个很好的性质，可以加速收敛。</p>
<p>逻辑回归可以把weight matrix初始化为0吗？ -可以。神经网络不可以。具体可见这篇博客：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/hejunlin1992/p/13022391.html">ref1./</a></p>
<h3 id="激活函数">激活函数</h3>
<p>指&lt;<strong>在整个定义域范围内</strong>&gt;的非线性函数。 sigmoid：值域（0，1），符合门控的定义，这也是为什么很多常见包含门控单元的模型中（GRU，LSTM等）都倾向于选择sigmoid作为激活函数。</p>
<p>tanh：值域（-1，1），多数情况下特征分布以0为中心，这种时候采用tanh比较好。且它在接近0的位置有着更大的梯度，因此收敛的会比sigmoid更快一些。</p>
<p>&lt;<strong>但是以上两种在多层的神经网络中都会带来梯度消失/爆炸的问题</strong>&gt;</p>
<p>如何缓解梯度下降的问题：</p>
<p>1）模型的greedy layer-wise pre-trained。指每两层作为一个base model单位进行训练，更新这两层的权重，用更靠后的一层作为下一个base model的输入，再更新两层之间的权重。</p>
<p>2）使用ReLU这种更接近线性函数的激活函数</p>
<h3 id="参数的最大似然估计mle和最大后验估计map">参数的最大似然估计（MLE）和最大后验估计（MAP）</h3>
<p>MLE：根据观测值直接估计参数的分布</p>
<p>MAP：观测值的基础上加上经验值&lt;<strong>先验概率（prior）</strong>&gt;</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46737512">ref2./</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/04/SOM-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/lukeandartoo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Air can sing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/04/SOM-learning/" itemprop="url">SOM-learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-02-04T03:25:22+02:00">
                2021-02-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/coding/" itemprop="url" rel="index">
                    <span itemprop="name">coding</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/coding/ml-dl/" itemprop="url" rel="index">
                    <span itemprop="name">ml&dl</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p align="right">
<font color="grey"> <i>——2021.2.4</i> </font>
</p>
<hr>
<h2 id="useful-outer-materials">useful outer materials:</h2>
<p><a target="_blank" rel="noopener" href="https://xubin.blog.csdn.net/article/details/50826892">ref1</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xbinworld/article/details/50818803">ref2</a></p>
<h2 id="what-it-can-do">What it can do:</h2>
<p>Dimension reduction</p>
<p>Clustering</p>
<p>Neighbourhood preserving maps</p>
<h2 id="why-soms-is-not-for-classification">Why SOMs is not for classification:</h2>
<p>It just generates the NEIGHBOURHOOD instead of class. If you want to use SOMs as classifier you need to add another classification layer.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/lukeandartoo.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7Carchive">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="rigel_6918@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">风鸣北川_rigel</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
